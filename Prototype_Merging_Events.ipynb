{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.special import comb\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts on how to reduce runtime: \n",
    "\n",
    "1. Use only interesting frames\n",
    "\n",
    "Pseudo Code: \n",
    "\n",
    "```Python\n",
    "interesting_frames = set()\n",
    "\n",
    "for _, event in events_df.iterrows():\n",
    "    if event[\"type.name\"] in [\"Pass\", \"Carry\", \"Shot\"]:\n",
    "        sec = event[\"second\"] + 60 * event[\"minute\"]\n",
    "        window_start = sec - 5\n",
    "        window_end = sec + 5\n",
    "\n",
    "        # Convert seconds â†’ frame indices\n",
    "        frame_start = int(window_start * frame_rate)\n",
    "        frame_end = int(window_end * frame_rate)\n",
    "\n",
    "        for f in range(frame_start, frame_end + 1):\n",
    "            interesting_frames.add(f) \n",
    "```\n",
    "\n",
    "Then slice your players_df:\n",
    "\n",
    "```Python\n",
    "players_df_window = players_df[\n",
    "    players_df[\"frameIdx\"].isin(interesting_frames)\n",
    "]\n",
    "```\n",
    "\n",
    "2. Use Random Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRACKING_DIR = pathlib.Path(\"tracking-compressed\")\n",
    "json_gz_paths = sorted(TRACKING_DIR.glob(\"tracking_*.json.gz\"))\n",
    "\n",
    "n_files = len(json_gz_paths) # or set to len(json_gz_paths) to load all\n",
    "\n",
    "# frames = []\n",
    "# players = []\n",
    "# used_match_ids = []  # <- store used match_ids here\n",
    "\n",
    "# for file_idx, json_gz_path in enumerate(json_gz_paths[:n_files]):\n",
    "#     match_id = json_gz_path.stem  # e.g. \"tracking_g2444470\"\n",
    "#     used_match_ids.append(match_id)  # <- keep track of what's loaded\n",
    "\n",
    "#     records = []\n",
    "\n",
    "#     with gzip.open(json_gz_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             records.append(json.loads(line))\n",
    "\n",
    "#     for r in records:\n",
    "#         f_data = {\n",
    "#             \"match_id\": match_id,\n",
    "#             \"period\": r[\"period\"],\n",
    "#             \"frameIdx\": r[\"frameIdx\"],\n",
    "#             \"gameClock\": r[\"gameClock\"],\n",
    "#             \"lastTouch_team\": r[\"lastTouch\"],\n",
    "#             \"ball_x\": r[\"ball\"][\"xyz\"][0],\n",
    "#             \"ball_y\": r[\"ball\"][\"xyz\"][1],\n",
    "#             \"ball_z\": r[\"ball\"][\"xyz\"][2],\n",
    "#         }\n",
    "#         frames.append(f_data)\n",
    "\n",
    "#         for side in [\"homePlayers\", \"awayPlayers\"]:\n",
    "#             for p in r[side]:\n",
    "#                 px, py, pz = p[\"xyz\"]\n",
    "#                 players.append({\n",
    "#                     \"match_id\": match_id,\n",
    "#                     \"period\": r[\"period\"],\n",
    "#                     \"frameIdx\": r[\"frameIdx\"],\n",
    "#                     \"side\": \"home\" if side == \"homePlayers\" else \"away\",\n",
    "#                     \"playerId\": p[\"playerId\"],\n",
    "#                     \"optaId\": str(p[\"optaId\"]),\n",
    "#                     \"number\": p[\"number\"],\n",
    "#                     \"x\": px, \"y\": py, \"z\": pz,\n",
    "#                     \"speed\": p[\"speed\"],\n",
    "#                 })\n",
    "\n",
    "# # Convert to DataFrames\n",
    "# frames_df = pd.DataFrame(frames)\n",
    "# players_df = pd.DataFrame(players)\n",
    "  \n",
    "frames = []\n",
    "players = []\n",
    "used_match_ids = []\n",
    "\n",
    "for file_idx, json_gz_path in enumerate(json_gz_paths[:n_files]):\n",
    "    match_id = json_gz_path.stem\n",
    "    used_match_ids.append(match_id)\n",
    "\n",
    "    # First pass: count lines\n",
    "    with gzip.open(json_gz_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "\n",
    "    # Randomly select frame indices\n",
    "    n_sampled_frames = 500\n",
    "    sampled_indices = set(\n",
    "        np.random.choice(\n",
    "            total_lines,\n",
    "            size=min(n_sampled_frames, total_lines),\n",
    "            replace=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Second pass: read only sampled frames\n",
    "    with gzip.open(json_gz_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i not in sampled_indices:\n",
    "                continue\n",
    "\n",
    "            r = json.loads(line)\n",
    "\n",
    "            f_data = {\n",
    "                \"match_id\": match_id,\n",
    "                \"period\": r[\"period\"],\n",
    "                \"frameIdx\": r[\"frameIdx\"],\n",
    "                \"gameClock\": r[\"gameClock\"],\n",
    "                \"lastTouch_team\": r[\"lastTouch\"],\n",
    "                \"ball_x\": r[\"ball\"][\"xyz\"][0],\n",
    "                \"ball_y\": r[\"ball\"][\"xyz\"][1],\n",
    "                \"ball_z\": r[\"ball\"][\"xyz\"][2],\n",
    "            }\n",
    "            frames.append(f_data)\n",
    "\n",
    "            for side in [\"homePlayers\", \"awayPlayers\"]:\n",
    "                for p in r[side]:\n",
    "                    px, py, pz = p[\"xyz\"]\n",
    "                    players.append({\n",
    "                        \"match_id\": match_id,\n",
    "                        \"period\": r[\"period\"],\n",
    "                        \"frameIdx\": r[\"frameIdx\"],\n",
    "                        \"side\": \"home\" if side == \"homePlayers\" else \"away\",\n",
    "                        \"playerId\": p[\"playerId\"],\n",
    "                        \"optaId\": str(p[\"optaId\"]),\n",
    "                        \"number\": p[\"number\"],\n",
    "                        \"x\": px, \"y\": py, \"z\": pz,\n",
    "                        \"speed\": p[\"speed\"],\n",
    "                    })\n",
    "\n",
    "frames_df = pd.DataFrame(frames)\n",
    "players_df = pd.DataFrame(players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_tracking_segments(players_df, frames_df,\n",
    "                             num_segments=10,\n",
    "                             segment_duration_sec=5,\n",
    "                             frame_rate=25.0,\n",
    "                             random_state=None):\n",
    "    \"\"\"\n",
    "    Randomly samples time segments from tracking data.\n",
    "\n",
    "    Returns:\n",
    "        players_df_sampled, frames_df_sampled\n",
    "    \"\"\"\n",
    "\n",
    "    sampled_frames = set()\n",
    "\n",
    "    for (match_id, period), group in frames_df.groupby([\"match_id\", \"period\"]):\n",
    "\n",
    "        max_frame = group[\"frameIdx\"].max()\n",
    "        total_seconds = max_frame / frame_rate\n",
    "\n",
    "        if total_seconds < segment_duration_sec:\n",
    "            continue  # skip short halves\n",
    "\n",
    "        # Possible start times in seconds\n",
    "        possible_starts = np.arange(\n",
    "            0,\n",
    "            total_seconds - segment_duration_sec,\n",
    "            step=1\n",
    "        )\n",
    "\n",
    "        if len(possible_starts) == 0:\n",
    "            continue\n",
    "\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        chosen_starts_sec = rng.choice(\n",
    "            possible_starts,\n",
    "            size=min(num_segments, len(possible_starts)),\n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        for start_sec in chosen_starts_sec:\n",
    "            start_frame = int(start_sec * frame_rate)\n",
    "            end_frame = start_frame + int(segment_duration_sec * frame_rate)\n",
    "\n",
    "            frames_in_segment = range(start_frame, end_frame + 1)\n",
    "            sampled_frames.update(\n",
    "                (match_id, period, f) for f in frames_in_segment\n",
    "            )\n",
    "\n",
    "    # Filter players_df and frames_df\n",
    "    sampled_frames_df = pd.DataFrame(\n",
    "        list(sampled_frames),\n",
    "        columns=[\"match_id\", \"period\", \"frameIdx\"]\n",
    "    )\n",
    "\n",
    "    players_df_sampled = players_df.merge(\n",
    "        sampled_frames_df,\n",
    "        on=[\"match_id\", \"period\", \"frameIdx\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    frames_df_sampled = frames_df.merge(\n",
    "        sampled_frames_df,\n",
    "        on=[\"match_id\", \"period\", \"frameIdx\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    return players_df_sampled, frames_df_sampled\n",
    "\n",
    "# Sample tracking segments\n",
    "players_df, frames_df = sample_tracking_segments(\n",
    "    players_df,\n",
    "    frames_df,\n",
    "    num_segments=10,\n",
    "    segment_duration_sec=5,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      events_file  match_date  \\\n",
      "0  statsbomb_pl_data/3837230.json  2022-08-05   \n",
      "1  statsbomb_pl_data/3837231.json  2022-08-06   \n",
      "2  statsbomb_pl_data/3837232.json  2022-08-07   \n",
      "3  statsbomb_pl_data/3837233.json  2022-08-06   \n",
      "4  statsbomb_pl_data/3837234.json  2022-08-06   \n",
      "\n",
      "                                team_names  \n",
      "0                [Arsenal, Crystal Palace]  \n",
      "1  [Leeds United, Wolverhampton Wanderers]  \n",
      "2              [Brentford, Leicester City]  \n",
      "3    [Newcastle United, Nottingham Forest]  \n",
      "4         [Southampton, Tottenham Hotspur]  \n"
     ]
    }
   ],
   "source": [
    "def extract_event_metadata(events_path):\n",
    "    teams = set()\n",
    "    match_date = None\n",
    "\n",
    "    with open(events_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            \n",
    "            # Grab match date\n",
    "            if match_date is None and \"match_date\" in obj:\n",
    "                match_date = obj[\"match_date\"]\n",
    "\n",
    "            # Grab team names\n",
    "            if \"team\" in obj and obj[\"team\"] is not None:\n",
    "                team_name = obj[\"team\"][\"name\"]\n",
    "                teams.add(team_name)\n",
    "            if \"possession_team\" in obj and obj[\"possession_team\"] is not None:\n",
    "                team_name = obj[\"possession_team\"][\"name\"]\n",
    "                teams.add(team_name)\n",
    "\n",
    "    return {\n",
    "        \"events_file\": str(events_path),\n",
    "        \"match_date\": match_date,\n",
    "        \"team_names\": list(teams),\n",
    "    }\n",
    "\n",
    "# Assume your event files are in a directory named 'events'\n",
    "EVENTS_DIR = pathlib.Path(\"statsbomb_pl_data\")\n",
    "event_files = sorted(EVENTS_DIR.glob(\"*.json\"))\n",
    "\n",
    "event_metadata_records = []\n",
    "\n",
    "for event_file in event_files:\n",
    "    meta = extract_event_metadata(event_file)\n",
    "    event_metadata_records.append(meta)\n",
    "\n",
    "event_meta_df = pd.DataFrame(event_metadata_records)\n",
    "\n",
    "print(event_meta_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       metadata_path tracking_suffix  \\\n",
      "0     metadata_SecondSpectrum/metadata_g2444536.json        g2444536   \n",
      "1  metadata_SecondSpectrum/g2292852_SecondSpectru...        g2292852   \n",
      "2     metadata_SecondSpectrum/metadata_g2444473.json        g2444473   \n",
      "3  metadata_SecondSpectrum/g2293073_SecondSpectru...        g2293073   \n",
      "4  metadata_SecondSpectrum/g2367750_SecondSpectru...        g2367750   \n",
      "\n",
      "   match_date home_team away_team  \n",
      "0  2024-10-05       EVE       NEW  \n",
      "1  2022-08-30       CRY       BRE  \n",
      "2  2024-08-17       IPS       LIV  \n",
      "3  2023-03-12       FUL       ARS  \n",
      "4  2024-01-30       FUL       EVE  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Collect all metadata files\n",
    "metadata_dir = pathlib.Path(\"metadata_SecondSpectrum\")\n",
    "all_metadata_files = list(metadata_dir.glob(\"*.json\"))\n",
    "\n",
    "tracking_meta_records = []\n",
    "\n",
    "for path in all_metadata_files:\n",
    "    with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    # Build match_date from year, month, day\n",
    "    if all(k in meta for k in [\"year\", \"month\", \"day\"]):\n",
    "        match_date = f\"{meta['year']:04}-{meta['month']:02}-{meta['day']:02}\"\n",
    "    else:\n",
    "        match_date = None\n",
    "\n",
    "    # Parse teams from description\n",
    "    desc = meta.get(\"description\", \"\")\n",
    "    home_team, away_team = None, None\n",
    "    if \" - \" in desc:\n",
    "        teams_part = desc.split(\":\")[0].strip()\n",
    "        home_team, away_team = teams_part.split(\" - \")\n",
    "\n",
    "    # Get tracking suffix from filename\n",
    "    if path.stem.startswith(\"metadata_g\"):\n",
    "        suffix = path.stem.split(\"_\")[1]\n",
    "    else:\n",
    "        suffix = path.stem.split(\"_\")[0]\n",
    "\n",
    "    tracking_meta_records.append({\n",
    "        \"metadata_path\": str(path),\n",
    "        \"tracking_suffix\": suffix,\n",
    "        \"match_date\": match_date,\n",
    "        \"home_team\": home_team,\n",
    "        \"away_team\": away_team,\n",
    "    })\n",
    "\n",
    "tracking_meta_df = pd.DataFrame(tracking_meta_records)\n",
    "\n",
    "print(tracking_meta_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TEAM CODE MAP:\n",
      "{'FUL': 'Fulham', 'BRE': 'Brentford', 'CRY': 'Crystal Palace', 'TOT': 'Tottenham Hotspur', 'BOU': 'AFC Bournemouth', 'SOU': 'Southampton', 'AVL': 'Aston Villa', 'WHU': 'West Ham United', 'MUN': 'Manchester United', 'ARS': 'Arsenal', 'LEI': 'Leicester City', 'NEW': 'Newcastle United', 'BHA': 'Brighton & Hove Albion', 'IPS': 'Ipswich Town', 'EVE': 'Everton', 'LIV': 'Liverpool', 'LEE': 'Leeds United', 'NOT': 'Nottingham Forest', 'MCI': 'Manchester City', 'WOL': 'Wolverhampton Wanderers', 'SHU': 'Sheffield United', 'CHE': 'Chelsea', 'LUT': 'Luton Town', 'BUR': 'Burnley'}\n"
     ]
    }
   ],
   "source": [
    "team_code_map = {'FUL': 'Fulham',\n",
    "                'BRE': 'Brentford',\n",
    "                'CRY': 'Crystal Palace', \n",
    "                'TOT': 'Tottenham Hotspur', \n",
    "                'BOU': 'AFC Bournemouth', \n",
    "                'SOU': 'Southampton',\n",
    "                'AVL': 'Aston Villa', \n",
    "                'WHU': 'West Ham United', \n",
    "                'MUN': 'Manchester United',\n",
    "                'ARS': 'Arsenal', \n",
    "                'LEI': 'Leicester City',\n",
    "                'NEW': 'Newcastle United',\n",
    "                'BHA': 'Brighton & Hove Albion',\n",
    "                'IPS': 'Ipswich Town', \n",
    "                'EVE': 'Everton', \n",
    "                'LIV': 'Liverpool',\n",
    "                'LEE': 'Leeds United', \n",
    "                'NOT': 'Nottingham Forest',\n",
    "                'MCI': 'Manchester City', \n",
    "                'WOL': 'Wolverhampton Wanderers',\n",
    "                'SHU': 'Sheffield United',\n",
    "                'CHE': 'Chelsea', \n",
    "                'LUT': 'Luton Town', \n",
    "                'BUR': 'Burnley'}\n",
    "\n",
    "print(\" TEAM CODE MAP:\")\n",
    "print(team_code_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      events_file  match_date               team_names  \\\n",
      "0  statsbomb_pl_data/3837230.json  2022-08-05                  Arsenal   \n",
      "1  statsbomb_pl_data/3837230.json  2022-08-05           Crystal Palace   \n",
      "2  statsbomb_pl_data/3837231.json  2022-08-06             Leeds United   \n",
      "3  statsbomb_pl_data/3837231.json  2022-08-06  Wolverhampton Wanderers   \n",
      "4  statsbomb_pl_data/3837232.json  2022-08-07                Brentford   \n",
      "\n",
      "                 team_name                                 key  \\\n",
      "0                  Arsenal                  2022-08-05_Arsenal   \n",
      "1           Crystal Palace           2022-08-05_Crystal Palace   \n",
      "2             Leeds United             2022-08-06_Leeds United   \n",
      "3  Wolverhampton Wanderers  2022-08-06_Wolverhampton Wanderers   \n",
      "4                Brentford                2022-08-07_Brentford   \n",
      "\n",
      "                                       metadata_path tracking_suffix  \\\n",
      "0  metadata_SecondSpectrum/g2292810_SecondSpectru...        g2292810   \n",
      "1  metadata_SecondSpectrum/g2292810_SecondSpectru...        g2292810   \n",
      "2  metadata_SecondSpectrum/g2292814_SecondSpectru...        g2292814   \n",
      "3  metadata_SecondSpectrum/g2292814_SecondSpectru...        g2292814   \n",
      "4  metadata_SecondSpectrum/g2292815_SecondSpectru...        g2292815   \n",
      "\n",
      "  match_date_tracking home_team away_team  home_team_full  \\\n",
      "0          2022-08-05       CRY       ARS  Crystal Palace   \n",
      "1          2022-08-05       CRY       ARS  Crystal Palace   \n",
      "2          2022-08-06       LEE       WOL    Leeds United   \n",
      "3          2022-08-06       LEE       WOL    Leeds United   \n",
      "4          2022-08-07       LEI       BRE  Leicester City   \n",
      "\n",
      "            away_team_full       team_name_tracking  \n",
      "0                  Arsenal                  Arsenal  \n",
      "1                  Arsenal           Crystal Palace  \n",
      "2  Wolverhampton Wanderers             Leeds United  \n",
      "3  Wolverhampton Wanderers  Wolverhampton Wanderers  \n",
      "4                Brentford                Brentford  \n"
     ]
    }
   ],
   "source": [
    "tracking_meta_df[\"home_team_full\"] = tracking_meta_df[\"home_team\"].map(team_code_map)\n",
    "tracking_meta_df[\"away_team_full\"] = tracking_meta_df[\"away_team\"].map(team_code_map)\n",
    "\n",
    "tracking_meta_long = pd.concat([\n",
    "    tracking_meta_df.assign(team_name=tracking_meta_df[\"home_team_full\"]),\n",
    "    tracking_meta_df.assign(team_name=tracking_meta_df[\"away_team_full\"]),\n",
    "])\n",
    "\n",
    "tracking_meta_long[\"key\"] = (\n",
    "    tracking_meta_long[\"match_date\"].fillna(\"\") + \"_\" +\n",
    "    tracking_meta_long[\"team_name\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "event_meta_exploded = event_meta_df.explode(\"team_names\")\n",
    "event_meta_exploded[\"team_name\"] = event_meta_exploded[\"team_names\"].fillna(\"\")\n",
    "\n",
    "event_meta_exploded[\"key\"] = (\n",
    "    event_meta_exploded[\"match_date\"].fillna(\"\") + \"_\" +\n",
    "    event_meta_exploded[\"team_name\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "event_tracking_df = event_meta_exploded.merge(\n",
    "    tracking_meta_long,\n",
    "    on=\"key\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_tracking\")\n",
    ")\n",
    "\n",
    "print(event_tracking_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      events_file  match_date        team_names  \\\n",
      "0  statsbomb_pl_data/3837230.json  2022-08-05           Arsenal   \n",
      "1  statsbomb_pl_data/3837231.json  2022-08-06      Leeds United   \n",
      "2  statsbomb_pl_data/3837232.json  2022-08-07         Brentford   \n",
      "3  statsbomb_pl_data/3837233.json  2022-08-06  Newcastle United   \n",
      "4  statsbomb_pl_data/3837234.json  2022-08-06       Southampton   \n",
      "\n",
      "          team_name                          key  \\\n",
      "0           Arsenal           2022-08-05_Arsenal   \n",
      "1      Leeds United      2022-08-06_Leeds United   \n",
      "2         Brentford         2022-08-07_Brentford   \n",
      "3  Newcastle United  2022-08-06_Newcastle United   \n",
      "4       Southampton       2022-08-06_Southampton   \n",
      "\n",
      "                                       metadata_path tracking_suffix  \\\n",
      "0  metadata_SecondSpectrum/g2292810_SecondSpectru...        g2292810   \n",
      "1  metadata_SecondSpectrum/g2292814_SecondSpectru...        g2292814   \n",
      "2  metadata_SecondSpectrum/g2292815_SecondSpectru...        g2292815   \n",
      "3  metadata_SecondSpectrum/g2292816_SecondSpectru...        g2292816   \n",
      "4  metadata_SecondSpectrum/g2292817_SecondSpectru...        g2292817   \n",
      "\n",
      "  match_date_tracking home_team away_team     home_team_full  \\\n",
      "0          2022-08-05       CRY       ARS     Crystal Palace   \n",
      "1          2022-08-06       LEE       WOL       Leeds United   \n",
      "2          2022-08-07       LEI       BRE     Leicester City   \n",
      "3          2022-08-06       NEW       NOT   Newcastle United   \n",
      "4          2022-08-06       TOT       SOU  Tottenham Hotspur   \n",
      "\n",
      "            away_team_full team_name_tracking  \n",
      "0                  Arsenal            Arsenal  \n",
      "1  Wolverhampton Wanderers       Leeds United  \n",
      "2                Brentford          Brentford  \n",
      "3        Nottingham Forest   Newcastle United  \n",
      "4              Southampton        Southampton  \n"
     ]
    }
   ],
   "source": [
    "event_tracking_df_clean = (\n",
    "    event_tracking_df\n",
    "    .dropna(subset=[\"tracking_suffix\"])\n",
    "    .drop_duplicates(subset=[\"events_file\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(event_tracking_df_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved events_dfs_by_match to disk.\n"
     ]
    }
   ],
   "source": [
    "# Add absolute seconds column\n",
    "def add_event_timestamps(events_df):\n",
    "    events_df[\"minute\"] = events_df[\"minute\"].fillna(0).astype(int)\n",
    "    events_df[\"second\"] = events_df[\"second\"].fillna(0).astype(int)\n",
    "\n",
    "    if \"milliseconds\" in events_df.columns:\n",
    "        events_df[\"milliseconds\"] = events_df[\"milliseconds\"].fillna(0).astype(int)\n",
    "    else:\n",
    "        events_df[\"milliseconds\"] = 0\n",
    "\n",
    "    events_df[\"seconds_period\"] = (\n",
    "        events_df[\"minute\"] * 60 +\n",
    "        events_df[\"second\"] +\n",
    "        events_df[\"milliseconds\"] / 1000\n",
    "    )\n",
    "\n",
    "    return events_df\n",
    "\n",
    "def match_events_to_frames(events_df, frames_df_match):\n",
    "    \"\"\"\n",
    "    Assigns nearest frameIdx to each event.\n",
    "    \"\"\"\n",
    "    period_to_frame_times = {}\n",
    "    for period, group in frames_df_match.groupby(\"period\"):\n",
    "        period_to_frame_times[period] = group[[\"seconds_period\", \"frameIdx\"]].sort_values(\"seconds_period\")\n",
    "\n",
    "    assigned_frames = []\n",
    "    for _, e in events_df.iterrows():\n",
    "        period = e[\"period\"]\n",
    "        seconds_event = e[\"seconds_period\"]\n",
    "\n",
    "        if period not in period_to_frame_times:\n",
    "            assigned_frames.append(None)\n",
    "            continue\n",
    "\n",
    "        times = period_to_frame_times[period][\"seconds_period\"].values\n",
    "        frames = period_to_frame_times[period][\"frameIdx\"].values\n",
    "\n",
    "        # Find closest frame\n",
    "        idx = np.argmin(np.abs(times - seconds_event))\n",
    "        assigned_frame = frames[idx]\n",
    "        assigned_frames.append(assigned_frame)\n",
    "\n",
    "    events_df[\"frameIdx\"] = assigned_frames\n",
    "    return events_df\n",
    "\n",
    "\n",
    "def find_events_during_run(run_df, events_df):\n",
    "    \"\"\"\n",
    "    Given a single run DataFrame and events_df\n",
    "    returns list of events that overlap this run\n",
    "    \"\"\"\n",
    "    period = run_df[\"period\"].iloc[0]\n",
    "    start_frame = run_df[\"frameIdx\"].min()\n",
    "    end_frame = run_df[\"frameIdx\"].max()\n",
    "\n",
    "    overlapping_events = events_df[\n",
    "        (events_df[\"period\"] == period) &\n",
    "        (events_df[\"frameIdx\"] >= start_frame) &\n",
    "        (events_df[\"frameIdx\"] <= end_frame)\n",
    "    ]\n",
    "    return overlapping_events\n",
    "\n",
    "# for _, row in event_tracking_df_clean.iterrows():\n",
    "#     events_path = row[\"events_file\"]\n",
    "#     tracking_suffix = row[\"tracking_suffix\"]\n",
    "#     tracking_match_id = f\"tracking_{tracking_suffix}\"\n",
    "\n",
    "#     # Slice tracking data for this match\n",
    "#     frames_df_match = frames_df[frames_df[\"match_id\"] == tracking_match_id]\n",
    "#     players_df_match = players_df[players_df[\"match_id\"] == tracking_match_id]\n",
    "\n",
    "#     print(\"=\" * 100)\n",
    "#     print(f\" MATCH FOUND:\")\n",
    "#     print(f\"â†’ Events file:   {events_path}\")\n",
    "#     print(f\"â†’ Tracking file: {tracking_match_id}.json.gz\")\n",
    "#     print(f\"â†’ Match Date:    {row['match_date']}\")\n",
    "#     print(f\"â†’ Home Team:     {row['home_team']}\")\n",
    "#     print(f\"â†’ Away Team:     {row['away_team']}\")\n",
    "#     print(f\"â†’ Number of tracking frames: {len(frames_df_match)}\")\n",
    "#     print(f\"â†’ Number of tracking player rows: {len(players_df_match)}\")\n",
    "#     print(\"=\" * 100)\n",
    "\n",
    "#     # Load events JSON\n",
    "#     with open(events_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "#         event_rows = [json.loads(line) for line in f]\n",
    "\n",
    "#     events_df = pd.DataFrame(event_rows)\n",
    "#     #print(events_df.head())\n",
    "\n",
    "#         # Add absolute seconds column\n",
    "#     events_df = add_event_timestamps(events_df)\n",
    "\n",
    "#     # Compute seconds for frames\n",
    "#     PERIOD_DURATION_SEC = 45 * 60\n",
    "#     frames_df_match[\"seconds_period\"] = PERIOD_DURATION_SEC - frames_df_match[\"gameClock\"]\n",
    "\n",
    "#     # Assign nearest frameIdx to each event\n",
    "#     events_df = match_events_to_frames(events_df, frames_df_match)\n",
    "\n",
    "#     # Find overlapping events for each run in this match\n",
    "#     runs_in_match = final_runs_df[final_runs_df[\"match_id\"] == tracking_match_id]\n",
    "\n",
    "#     for run_id, run_df in runs_in_match.groupby(\"run_id\"):\n",
    "#         overlapping_events = find_events_during_run(run_df, events_df)\n",
    "\n",
    "#         if not overlapping_events.empty:\n",
    "#             print(f\"Run {run_id} overlaps with {len(overlapping_events)} events:\")\n",
    "#             print(overlapping_events[[\"type\", \"minute\", \"second\", \"player\", \"team\"]].head())\n",
    "\n",
    "# events_dfs_by_match = {}\n",
    "\n",
    "# for _, row in event_tracking_df_clean.iterrows():\n",
    "#     events_path = row[\"events_file\"]\n",
    "#     tracking_suffix = row[\"tracking_suffix\"]\n",
    "#     tracking_match_id = f\"tracking_{tracking_suffix}\"\n",
    "\n",
    "#     # Slice tracking data for this match\n",
    "#     frames_df_match = frames_df[frames_df[\"match_id\"] == tracking_match_id]\n",
    "#     players_df_match = players_df[players_df[\"match_id\"] == tracking_match_id]\n",
    "\n",
    "#     # print(\"=\" * 100)\n",
    "#     # print(f\" MATCH FOUND:\")\n",
    "#     # print(f\"â†’ Events file:   {events_path}\")\n",
    "#     # print(f\"â†’ Tracking file: {tracking_match_id}.json.gz\")\n",
    "#     # print(f\"â†’ Match Date:    {row['match_date']}\")\n",
    "#     # print(f\"â†’ Home Team:     {row['home_team']}\")\n",
    "#     # print(f\"â†’ Away Team:     {row['away_team']}\")\n",
    "#     # print(f\"â†’ Number of tracking frames: {len(frames_df_match)}\")\n",
    "#     # print(f\"â†’ Number of tracking player rows: {len(players_df_match)}\")\n",
    "#     # print(\"=\" * 100)\n",
    "\n",
    "#     # Load events JSON\n",
    "#     with open(events_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "#         event_rows = [json.loads(line) for line in f]\n",
    "#     events_df = pd.DataFrame(event_rows)\n",
    "    \n",
    "#     # add timestamps\n",
    "#     events_df = add_event_timestamps(events_df)\n",
    "\n",
    "#     # attach frameIdx\n",
    "#     frames_df_match[\"seconds_period\"] = 45*60 - frames_df_match[\"gameClock\"]\n",
    "#     events_df = match_events_to_frames(events_df, frames_df_match)\n",
    "\n",
    "#     # store this DataFrame for later:\n",
    "#     events_dfs_by_match[tracking_match_id] = events_df\n",
    "\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"events_dfs_by_match.joblib\"):\n",
    "    events_dfs_by_match = joblib.load(\"events_dfs_by_match.joblib\")\n",
    "    print(\"âœ… Loaded precomputed events_dfs_by_match.\")\n",
    "else:\n",
    "    # run your original block:\n",
    "    events_dfs_by_match = {}\n",
    "\n",
    "    for _, row in event_tracking_df_clean.iterrows():\n",
    "        events_path = row[\"events_file\"]\n",
    "        tracking_suffix = row[\"tracking_suffix\"]\n",
    "        tracking_match_id = f\"tracking_{tracking_suffix}\"\n",
    "\n",
    "        frames_df_match = frames_df[frames_df[\"match_id\"] == tracking_match_id]\n",
    "        players_df_match = players_df[players_df[\"match_id\"] == tracking_match_id]\n",
    "\n",
    "        with open(events_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            event_rows = [json.loads(line) for line in f]\n",
    "\n",
    "        events_df = pd.DataFrame(event_rows)\n",
    "        events_df = add_event_timestamps(events_df)\n",
    "\n",
    "        frames_df_match[\"seconds_period\"] = 45*60 - frames_df_match[\"gameClock\"]\n",
    "        events_df = match_events_to_frames(events_df, frames_df_match)\n",
    "\n",
    "        events_dfs_by_match[tracking_match_id] = events_df\n",
    "\n",
    "    joblib.dump(events_dfs_by_match, \"events_dfs_by_match.joblib\")\n",
    "    print(\"âœ… Saved events_dfs_by_match to disk.\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# REMEMBER TO ADD THIS AFTER FINAL_RUNS_DF IS DEFINED\n",
    "# for match_id, events_df in events_dfs_by_match.items():\n",
    "#     runs_in_match = final_runs_df[final_runs_df[\"match_id\"] == match_id]\n",
    "\n",
    "#     for run_id, run_df in runs_in_match.groupby(\"run_id\"):\n",
    "#         overlapping_events = find_events_during_run(run_df, events_df)\n",
    "\n",
    "#         if not overlapping_events.empty:\n",
    "#             print(f\"Run {run_id} overlaps with {len(overlapping_events)} events:\")\n",
    "#             print(overlapping_events[[\"type\", \"minute\", \"second\", \"player\", \"team\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'index', 'period', 'timestamp', 'minute', 'second', 'type',\n",
      "       'possession', 'possession_team', 'play_pattern', 'obv_for_after',\n",
      "       'obv_for_before', 'obv_for_net', 'obv_against_after',\n",
      "       'obv_against_before', 'obv_against_net', 'obv_total_net', 'team',\n",
      "       'duration', 'tactics', 'load_datetime', 'user', 'match_date',\n",
      "       'match_id', 'pipeline_run_id', 'extraction_timestamp', 'related_events',\n",
      "       'player', 'position', 'location', 'pass', 'carry', 'ball_receipt',\n",
      "       'under_pressure', 'duel', 'counterpress', 'interception', 'out',\n",
      "       'dribble', 'ball_recovery', 'off_camera', 'shot', 'goalkeeper',\n",
      "       'clearance', 'block', 'foul_committed', 'foul_won', 'miscontrol',\n",
      "       'substitution', 'milliseconds', 'seconds_period', 'frameIdx'],\n",
      "      dtype='object')\n",
      "(3806, 52)\n"
     ]
    }
   ],
   "source": [
    "sample_match_id = list(events_dfs_by_match.keys())[0]\n",
    "\n",
    "# Grab its DataFrame\n",
    "events_df = events_dfs_by_match[sample_match_id]\n",
    "\n",
    "# View basic info\n",
    "#print(events_df.head(20))\n",
    "print(events_df.columns)\n",
    "print(events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded metadata for 45405 players.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract suffixes like \"g2444470\" from used tracking files\n",
    "used_match_suffixes = [match_id.split(\"_\", 1)[1].replace(\".json\", \"\") for match_id in used_match_ids]\n",
    "\n",
    "# Step 2: Gather metadata file paths from both formats\n",
    "metadata_dir = pathlib.Path(\"metadata_SecondSpectrum\")\n",
    "all_metadata_files = list(metadata_dir.glob(\"*.json\"))\n",
    "\n",
    "# Build map: match_suffix (e.g., \"g2444470\") â†’ metadata_path\n",
    "metadata_file_map = {}\n",
    "for path in all_metadata_files:\n",
    "    filename = path.name\n",
    "    if filename.startswith(\"metadata_g\") and filename.endswith(\".json\"):\n",
    "        suffix = filename.split(\"_\")[1].split(\".\")[0]  # 'g2444470'\n",
    "    elif filename.endswith(\"_SecondSpectrum_Metadata.json\"):\n",
    "        suffix = filename.split(\"_\")[0]  # 'g2444470'\n",
    "    else:\n",
    "        continue  # skip non-matching files\n",
    "    metadata_file_map[suffix] = path\n",
    "\n",
    "# Build a lookup DataFrame linking tracking suffixes to match date and teams\n",
    "tracking_meta_records = []\n",
    "for suffix, metadata_path in metadata_file_map.items():\n",
    "    with open(metadata_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        meta = json.load(f)\n",
    "    tracking_meta_records.append({\n",
    "        \"match_date\": meta.get(\"matchDate\"),\n",
    "        \"home_team\": meta.get(\"homeTeamName\"),\n",
    "        \"away_team\": meta.get(\"awayTeamName\"),\n",
    "        \"tracking_suffix\": suffix,\n",
    "        \"metadata_path\": str(metadata_path),\n",
    "    })\n",
    "\n",
    "\n",
    "# Step 3: Load metadata and build lookup for used matches\n",
    "opta_meta_lookup = {}\n",
    "\n",
    "for suffix in used_match_suffixes:\n",
    "    metadata_path = metadata_file_map.get(suffix)\n",
    "    if not metadata_path:\n",
    "        print(f\" No metadata found for match {suffix}\")\n",
    "        continue\n",
    "\n",
    "    with open(metadata_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    match_id = f\"tracking_{suffix}\"  # same format as tracking match_id\n",
    "\n",
    "    for side, team in [(\"homePlayers\", \"home\"), (\"awayPlayers\", \"away\")]:\n",
    "        for p in meta.get(side, []):\n",
    "            key = (match_id, str(p[\"optaId\"]))\n",
    "            opta_meta_lookup[key] = {\n",
    "                \"player_name\": p.get(\"name\"),\n",
    "                \"position\": p.get(\"position\"),\n",
    "                \"team_role\": team,\n",
    "            }\n",
    "\n",
    "print(f\" Loaded metadata for {len(opta_meta_lookup)} players.\")\n",
    "\n",
    "meta_df = pd.DataFrame([\n",
    "    {\n",
    "        \"match_id\": match_id,\n",
    "        \"optaId\": opta_id,\n",
    "        \"player_name\": info[\"player_name\"],\n",
    "        \"position\": info[\"position\"],\n",
    "        \"team_role\": info[\"team_role\"],\n",
    "    }\n",
    "    for (match_id, opta_id), info in opta_meta_lookup.items()\n",
    "])\n",
    "\n",
    "players_df[\"match_id_clean\"] = players_df[\"match_id\"].str.replace(\".json\", \"\", regex=False)\n",
    "\n",
    "# Merge using match_id and optaId as keys\n",
    "players_df = players_df.merge(\n",
    "    meta_df,\n",
    "    how=\"left\",\n",
    "    left_on=[\"match_id_clean\", \"optaId\"],\n",
    "    right_on=[\"match_id\", \"optaId\"]\n",
    ")\n",
    "\n",
    "players_df.drop(columns=[\"match_id_clean\", \"match_id_y\"], inplace=True)\n",
    "players_df.rename(columns={\"match_id_x\": \"match_id\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runs segmented: 31950\n",
      "Total off-ball runs (with min distance): 29367\n",
      "          player_name position team_role      x      y  speed\n",
      "0  Gabriel Martinelli       LW      away -33.22  -6.45   3.07\n",
      "1  Gabriel Martinelli       LW      away  -6.59  -3.36   2.02\n",
      "2           W. Saliba      RCB      away   0.08  10.02   3.41\n",
      "3           T. Partey      CDM      away  -9.98  15.96   4.61\n",
      "4            M. Guehi      LCB      home -47.19  16.65   2.55\n"
     ]
    }
   ],
   "source": [
    "# Segmenting Runs (across multiple matches)\n",
    "def segment_runs(players_df, speed_threshold=2.0):\n",
    "    \"\"\"\n",
    "    Segments continuous runs for each player within each match and period\n",
    "    when speed exceeds a threshold.\n",
    "    \"\"\"\n",
    "    runs = []\n",
    "    for (match_id, period, playerId), group in players_df.groupby([\"match_id\", \"period\", \"playerId\"]):\n",
    "        group = group.sort_values(\"frameIdx\")\n",
    "        current_run = []\n",
    "        for _, row in group.iterrows():\n",
    "            if row[\"speed\"] > speed_threshold:\n",
    "                current_run.append(row)\n",
    "            elif current_run:\n",
    "                runs.append(pd.DataFrame(current_run))\n",
    "                current_run = []\n",
    "        if current_run:\n",
    "            runs.append(pd.DataFrame(current_run))\n",
    "    return runs\n",
    "\n",
    "def filter_off_ball_runs_with_distance(runs_list, frames_df, players_df, min_distance=3.0):\n",
    "    \"\"\"\n",
    "    Filters runs to keep only those where:\n",
    "    - The player never touched the ball (not lastTouch)\n",
    "    - The player is always at least `min_distance` away from the ball\n",
    "    \"\"\"\n",
    "    frame_last_touch = frames_df.set_index([\"match_id\", \"period\", \"frameIdx\"])[\"lastTouch_team\"].to_dict()\n",
    "    ball_positions = frames_df.set_index([\"match_id\", \"period\", \"frameIdx\"])[[\"ball_x\", \"ball_y\"]].to_dict(\"index\")\n",
    "    \n",
    "    off_ball_runs = []\n",
    "\n",
    "    for run_df in runs_list:\n",
    "        player_id = run_df[\"playerId\"].iloc[0]\n",
    "        match_id = run_df[\"match_id\"].iloc[0]\n",
    "        period = run_df[\"period\"].iloc[0]\n",
    "        frame_idxs = run_df[\"frameIdx\"].values\n",
    "\n",
    "        is_off_ball = True\n",
    "        for frame_idx in frame_idxs:\n",
    "            key = (match_id, period, frame_idx)\n",
    "\n",
    "            # Check lastTouch\n",
    "            if frame_last_touch.get(key) == player_id:\n",
    "                is_off_ball = False\n",
    "                break\n",
    "\n",
    "            # Check distance from ball\n",
    "            ball_pos = ball_positions.get(key)\n",
    "            if ball_pos is None:\n",
    "                continue  # Skip frames with missing ball info\n",
    "\n",
    "            player_pos = run_df[run_df[\"frameIdx\"] == frame_idx][[\"x\", \"y\"]].values\n",
    "            if player_pos.size == 0:\n",
    "                continue\n",
    "\n",
    "            dist = np.linalg.norm(player_pos[0] - np.array([ball_pos[\"ball_x\"], ball_pos[\"ball_y\"]]))\n",
    "            if dist < min_distance:\n",
    "                is_off_ball = False\n",
    "                break\n",
    "\n",
    "        if is_off_ball:\n",
    "            off_ball_runs.append(run_df)\n",
    "\n",
    "    return off_ball_runs\n",
    "\n",
    "runs_list = segment_runs(players_df)\n",
    "print(f\"Total runs segmented: {len(runs_list)}\")\n",
    "\n",
    "runs_list = filter_off_ball_runs_with_distance(runs_list, frames_df, players_df, min_distance=3.0)\n",
    "print(f\"Total off-ball runs (with min distance): {len(runs_list)}\")\n",
    "\n",
    "# Annotate each run with player metadata\n",
    "annotated_runs = []\n",
    "\n",
    "for run_df in runs_list:\n",
    "    # Make a copy of the run to avoid modifying in-place\n",
    "    run_df = run_df.copy()\n",
    "\n",
    "    # Extract metadata from the first row (same for entire run)\n",
    "    meta_fields = [\"playerId\", \"optaId\", \"match_id\", \"player_name\", \"position\", \"team_role\"]\n",
    "    for field in meta_fields:\n",
    "        run_df[field] = run_df.iloc[0][field]\n",
    "\n",
    "    annotated_runs.append(run_df)\n",
    "\n",
    "# Assign a unique run_id to each run\n",
    "for i, run_df in enumerate(annotated_runs):\n",
    "    run_df[\"run_id\"] = i\n",
    "\n",
    "# Optional: Combine into one dataframe\n",
    "all_runs_df = pd.concat(annotated_runs, ignore_index=True)\n",
    "\n",
    "# Preview\n",
    "print(all_runs_df[[\"player_name\", \"position\", \"team_role\", \"x\", \"y\", \"speed\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kt/gmkq5dyj63s1r35swbwrg5jr0000gn/T/ipykernel_9866/2786365659.py:26: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_runs_df = all_runs_df.groupby(\"run_id\", group_keys=False).apply(mirror_group)\n"
     ]
    }
   ],
   "source": [
    "# Def mirror function\n",
    "def mirror_group(group):\n",
    "\n",
    "    y_mean = group[\"y\"].mean()\n",
    "    if y_mean < 0:\n",
    "        group[\"y_mirror\"] = -group[\"y\"]\n",
    "    else:\n",
    "        group[\"y_mirror\"] = group[\"y\"]\n",
    "    group[\"x_mirror\"] = group[\"x\"]\n",
    "\n",
    "\n",
    "    return group\n",
    "\n",
    "def should_flip_x(team_role, period):\n",
    "    \"\"\"\n",
    "    Returns True if this team in this period attacks right-to-left.\n",
    "    \"\"\"\n",
    "    if period == 1:\n",
    "        return team_role == \"away\"\n",
    "    elif period == 2:\n",
    "        return team_role == \"home\"\n",
    "    else:\n",
    "        return False  # Just in case\n",
    "\n",
    "# Apply mirroring per run\n",
    "all_runs_df = all_runs_df.groupby(\"run_id\", group_keys=False).apply(mirror_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroids computed for 13462 frame-side combinations.\n",
      "[(('tracking_g2292810.json', 1, 5904, 'home'), array([-22.19727273,  -0.88727273])), (('tracking_g2292810.json', 1, 5904, 'away'), array([-12.384,   0.21 ])), (('tracking_g2292810.json', 1, 6072, 'home'), array([-23.99545455,   2.95636364])), (('tracking_g2292810.json', 1, 6072, 'away'), array([-16.061,   6.767])), (('tracking_g2292810.json', 1, 44470, 'home'), array([-5.85545455, 15.73363636]))]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Precompute centroids for all frames and both teams (excluding goalkeeper)\n",
    "players_df[\"number\"] = players_df[\"number\"].astype(int)\n",
    "\n",
    "centroid_dict = {}\n",
    "for (match_id, period, frame_idx), group in players_df.groupby([\"match_id\", \"period\", \"frameIdx\"]):\n",
    "    for side in [\"home\", \"away\"]:\n",
    "        team_players = group[(group[\"side\"] == side) & (group[\"number\"] != 1)]\n",
    "        centroid = team_players[[\"x\", \"y\"]].mean().values if not team_players.empty else np.array([0.0, 0.0])\n",
    "        centroid_dict[(match_id, period, frame_idx, side)] = centroid\n",
    "\n",
    "print(f\"Centroids computed for {len(centroid_dict)} frame-side combinations.\")\n",
    "print(list(centroid_dict.items())[:5])  # show first few for inspection\n",
    "\n",
    "frame_last_touch_team = frames_df.set_index([\"match_id\", \"period\", \"frameIdx\"])[\"lastTouch_team\"].to_dict()\n",
    "player_side_lookup = players_df.set_index([\"match_id\", \"period\", \"frameIdx\", \"playerId\"])[\"side\"].to_dict()\n",
    "\n",
    "adjusted_runs_list = []\n",
    "\n",
    "grouped = all_runs_df.groupby([\"match_id\", \"period\", \"playerId\", \"run_id\"], group_keys=False)\n",
    "\n",
    "for _, run_df in grouped:\n",
    "    run_df = run_df.sort_values(\"frameIdx\")\n",
    "    match_id = run_df[\"match_id\"].iloc[0]\n",
    "    period = run_df[\"period\"].iloc[0]\n",
    "    start_frame = run_df[\"frameIdx\"].iloc[0]\n",
    "\n",
    "    team_centroid = np.array([0.0, 0.0])  # fallback default\n",
    "\n",
    "    key = (match_id, period, start_frame)\n",
    "    possession_side = frame_last_touch_team.get(key)\n",
    "\n",
    "    team_role = run_df[\"team_role\"].iloc[0]\n",
    "\n",
    "    if possession_side is None: \n",
    "        in_possession = np.nan\n",
    "        phase_of_play = np.nan\n",
    "    else: \n",
    "        in_possession = (team_role == possession_side)\n",
    "        phase_of_play = \"attack\" if in_possession else \"defend\" \n",
    "\n",
    "    run_df[\"in_possession\"] = in_possession\n",
    "    run_df[\"phase_of_play\"] = phase_of_play\n",
    "\n",
    "    # if possession_side is not None:\n",
    "    #     team_centroid = centroid_dict.get((match_id, period, start_frame, possession_side), team_centroid)\n",
    "\n",
    "    # Always compute centroid for this player's own team. As opposed to above where we used possession side.\n",
    "    team_centroid = centroid_dict.get(\n",
    "        (match_id, period, start_frame, team_role),\n",
    "        np.array([0.0, 0.0])\n",
    "    )\n",
    "\n",
    "    #print(team_centroid)\n",
    "\n",
    "    run_df[\"x_c\"] = run_df[\"x\"] - team_centroid[0]\n",
    "    run_df[\"y_c\"] = run_df[\"y\"] - team_centroid[1]\n",
    "    run_df[\"x_mirror_c\"] = run_df[\"x_mirror\"] - team_centroid[0]\n",
    "    run_df[\"y_mirror_c\"] = run_df[\"y_mirror\"] - team_centroid[1]\n",
    "\n",
    "    flip_x = should_flip_x(team_role, period)\n",
    "    if flip_x:\n",
    "        run_df[\"x_mirror\"] = -run_df[\"x_mirror\"]\n",
    "        run_df[\"x_mirror_c\"] = -run_df[\"x_mirror_c\"]\n",
    "\n",
    "    adjusted_runs_list.append(run_df)\n",
    "\n",
    "final_runs_df = pd.concat(adjusted_runs_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlapping runs\n",
    "\n",
    "def extract_ball_carrier_df_fast(run_df, players_with_ball_df):\n",
    "    \"\"\"\n",
    "    Fast version of extract_ball_carrier_df.\n",
    "    \"\"\"\n",
    "    match_id = run_df[\"match_id\"].iloc[0]\n",
    "    period = run_df[\"period\"].iloc[0]\n",
    "    team_role = run_df[\"team_role\"].iloc[0]\n",
    "    frames = run_df[\"frameIdx\"].unique()\n",
    "\n",
    "    # Slice relevant frames for this run\n",
    "    run_players = players_with_ball_df[\n",
    "        (players_with_ball_df[\"match_id\"] == match_id) &\n",
    "        (players_with_ball_df[\"period\"] == period) &\n",
    "        (players_with_ball_df[\"frameIdx\"].isin(frames))\n",
    "    ]\n",
    "\n",
    "    # Keep only teammates\n",
    "    teammates = run_players[\n",
    "        run_players[\"side\"] == team_role\n",
    "    ].copy()\n",
    "\n",
    "    if teammates.empty:\n",
    "        return None\n",
    "\n",
    "    # Compute distance to ball\n",
    "    teammates[\"dist_to_ball\"] = np.linalg.norm(\n",
    "        teammates[[\"x\", \"y\"]].values - teammates[[\"ball_x\", \"ball_y\"]].values,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Find closest player in each frame\n",
    "    idx_min_dist = teammates.groupby(\"frameIdx\")[\"dist_to_ball\"].idxmin()\n",
    "\n",
    "    ball_carrier_df = teammates.loc[idx_min_dist]\n",
    "\n",
    "    if ball_carrier_df.empty:\n",
    "        return None\n",
    "    else:\n",
    "        return ball_carrier_df\n",
    "\n",
    "def is_overlapping_run(run_df, ball_carrier_df, min_pass_distance=5.0):\n",
    "    \"\"\"\n",
    "    Checks if a run overlaps the ball carrier.\n",
    "    Skips runs if the ball carrier changes during the run.\n",
    "    \"\"\"\n",
    "    if ball_carrier_df is None:\n",
    "        return False\n",
    "\n",
    "    # Check how many unique carriers there were during this run\n",
    "    unique_carriers = ball_carrier_df[\"playerId\"].nunique()\n",
    "    # Can look at subRuns later, but for now we assume one carrier per run\n",
    "    if unique_carriers > 1:\n",
    "        # Carrier changed mid-run; skip for safety\n",
    "        return False\n",
    "\n",
    "    # Proceed as before\n",
    "    f_start = run_df[\"frameIdx\"].iloc[0]\n",
    "    f_end = run_df[\"frameIdx\"].iloc[-1]\n",
    "\n",
    "    runner_start = run_df.iloc[0][[\"x\", \"y\"]].values\n",
    "    runner_end = run_df.iloc[-1][[\"x\", \"y\"]].values\n",
    "\n",
    "    carrier_start_row = ball_carrier_df.loc[\n",
    "        ball_carrier_df[\"frameIdx\"] == f_start\n",
    "    ]\n",
    "\n",
    "    carrier_end_row = ball_carrier_df.loc[\n",
    "        ball_carrier_df[\"frameIdx\"] == f_end\n",
    "    ]\n",
    "\n",
    "    if carrier_start_row.empty or carrier_end_row.empty:\n",
    "        return False\n",
    "\n",
    "    carrier_start = carrier_start_row.iloc[0][[\"x\", \"y\"]].values\n",
    "    carrier_end = carrier_end_row.iloc[0][[\"x\", \"y\"]].values\n",
    "\n",
    "    # Compute lateral offset (y-direction)\n",
    "    delta_start_y = runner_start[1] - carrier_start[1]\n",
    "    delta_end_y = runner_end[1] - carrier_end[1]\n",
    "\n",
    "    # Check if runner switched sides outside the carrier\n",
    "    overlap_side_change = np.sign(delta_start_y) != np.sign(delta_end_y)\n",
    "\n",
    "    # Check lateral distance threshold\n",
    "    lateral_movement = abs(delta_end_y - delta_start_y)\n",
    "\n",
    "    # Ensure overlap moves forward enough\n",
    "    forward_distance = runner_end[0] - runner_start[0]\n",
    "\n",
    "    return (\n",
    "        overlap_side_change and\n",
    "        lateral_movement > 2.0 and\n",
    "        forward_distance > min_pass_distance\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Underlapping\n",
    "\n",
    "\n",
    "\n",
    "def is_underlapping_run(run_df, ball_carrier_df, min_pass_distance=5.0):\n",
    "    \"\"\"\n",
    "    Heuristic: detects underlapping runs where the runner cuts inside\n",
    "    relative to the ball carrier.\n",
    "\n",
    "    Returns True only if the ball carrier remains constant during the run.\n",
    "    \"\"\"\n",
    "    if ball_carrier_df is None:\n",
    "        return False\n",
    "\n",
    "    # Check how many unique carriers there were during this run\n",
    "    unique_carriers = ball_carrier_df[\"playerId\"].nunique()\n",
    "    if unique_carriers > 1:\n",
    "        # Carrier changed mid-run; skip for safety\n",
    "        return False\n",
    "\n",
    "    # Proceed as before\n",
    "    f_start = run_df[\"frameIdx\"].iloc[0]\n",
    "    f_end = run_df[\"frameIdx\"].iloc[-1]\n",
    "\n",
    "    runner_start = run_df.iloc[0][[\"x\", \"y\"]].values\n",
    "    runner_end = run_df.iloc[-1][[\"x\", \"y\"]].values\n",
    "\n",
    "    carrier_start_row = ball_carrier_df.loc[\n",
    "        ball_carrier_df[\"frameIdx\"] == f_start\n",
    "    ]\n",
    "\n",
    "    carrier_end_row = ball_carrier_df.loc[\n",
    "        ball_carrier_df[\"frameIdx\"] == f_end\n",
    "    ]\n",
    "\n",
    "    if carrier_start_row.empty or carrier_end_row.empty:\n",
    "        return False\n",
    "\n",
    "    carrier_start = carrier_start_row.iloc[0][[\"x\", \"y\"]].values\n",
    "    carrier_end = carrier_end_row.iloc[0][[\"x\", \"y\"]].values\n",
    "\n",
    "    # Compute lateral offsets (y-difference)\n",
    "    delta_start_y = runner_start[1] - carrier_start[1]\n",
    "    delta_end_y = runner_end[1] - carrier_end[1]\n",
    "\n",
    "    overlap_side_change = np.sign(delta_start_y) != np.sign(delta_end_y)\n",
    "\n",
    "    lateral_distance_start = abs(delta_start_y)\n",
    "    lateral_distance_end = abs(delta_end_y)\n",
    "\n",
    "    lateral_movement = lateral_distance_start - lateral_distance_end\n",
    "\n",
    "    forward_distance = runner_end[0] - runner_start[0]\n",
    "\n",
    "    # For underlap:\n",
    "    # - sign change\n",
    "    # - lateral distance reduced (runner cuts inside)\n",
    "    # - forward enough\n",
    "    return (\n",
    "        overlap_side_change\n",
    "        and lateral_movement > 1.0\n",
    "        and forward_distance > min_pass_distance\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_diagonal_run(run_df, min_length=5.0, angle_min=20, angle_max=70):\n",
    "    \"\"\"\n",
    "    Heuristic to detect diagonal runs.\n",
    "\n",
    "    - Checks total length\n",
    "    - Checks that the angle lies within a diagonal corridor\n",
    "\n",
    "    Returns True if diagonal.\n",
    "    \"\"\"\n",
    "    # Start and end positions\n",
    "    runner_start = run_df.iloc[0][[\"x\", \"y\"]].values\n",
    "    runner_end = run_df.iloc[-1][[\"x\", \"y\"]].values\n",
    "\n",
    "    delta_x = runner_end[0] - runner_start[0]\n",
    "    delta_y = runner_end[1] - runner_start[1]\n",
    "\n",
    "    # Total run distance\n",
    "    total_distance = np.linalg.norm([delta_x, delta_y])\n",
    "    if total_distance < min_length:\n",
    "        return False\n",
    "\n",
    "    # Compute angle in degrees\n",
    "    angle_deg = np.degrees(np.arctan2(delta_y, delta_x))\n",
    "\n",
    "    abs_angle = abs(angle_deg)\n",
    "\n",
    "    # Check if angle is in diagonal corridor\n",
    "    return angle_min <= abs_angle <= angle_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   run_id  start_zone  end_zone  start_zone_absolute  end_zone_absolute  \\\n",
      "0       0           5         5                    4                  5   \n",
      "1       1           5         5                    5                  5   \n",
      "2       2           5         5                    8                  8   \n",
      "3       3           7         7                    7                  7   \n",
      "4       4           8         8                    8                  8   \n",
      "\n",
      "  phase_of_play  in_possession team_role position  run_length_m  mean_speed  \\\n",
      "0        defend          False      away       LW     26.808674       2.545   \n",
      "1        defend          False      away      RCB      0.000000       3.410   \n",
      "2        defend          False      away      CDM      0.000000       4.610   \n",
      "3        attack           True      home      LCB      0.000000       2.550   \n",
      "4        attack           True      home       LW      0.000000       2.020   \n",
      "\n",
      "   max_speed  run_angle_deg  run_forward  tactical_overlap  tactical_underlap  \\\n",
      "0       3.07    -173.381309        False             False              False   \n",
      "1       3.41       0.000000        False             False              False   \n",
      "2       4.61       0.000000        False             False              False   \n",
      "3       2.55       0.000000        False             False              False   \n",
      "4       2.02       0.000000        False             False              False   \n",
      "\n",
      "   tactical_diagonal  \n",
      "0              False  \n",
      "1              False  \n",
      "2              False  \n",
      "3              False  \n",
      "4              False  \n"
     ]
    }
   ],
   "source": [
    "# Define grid\n",
    "x_edges = np.linspace(-52.5, 52.5, 4)   # splits pitch length into thirds\n",
    "y_edges = np.linspace(-34, 34, 4)       # splits width into thirds\n",
    "\n",
    "def get_zone(x, y, x_edges, y_edges):\n",
    "    x_bin = np.digitize([x], x_edges)[0] - 1\n",
    "    y_bin = np.digitize([y], y_edges)[0] - 1\n",
    "    x_bin = min(max(x_bin, 0), len(x_edges)-2)\n",
    "    y_bin = min(max(y_bin, 0), len(y_edges)-2)\n",
    "    zone_idx = y_bin * (len(x_edges)-1) + x_bin + 1\n",
    "    return zone_idx\n",
    "\n",
    "# Build players_with_ball_df ONCE for all runs\n",
    "players_with_ball_df = players_df.merge(\n",
    "    frames_df[[\"match_id\", \"period\", \"frameIdx\", \"ball_x\", \"ball_y\", \"lastTouch_team\"]],\n",
    "    on=[\"match_id\", \"period\", \"frameIdx\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_ball\")\n",
    ")\n",
    "\n",
    "zone_records = []\n",
    "\n",
    "for run_id, run_df in final_runs_df.groupby(\"run_id\"):\n",
    "\n",
    "    # Mirrored & centered positions\n",
    "    start_x_mirror_c = run_df[\"x_mirror_c\"].iloc[0]\n",
    "    start_y_mirror_c = run_df[\"y_mirror_c\"].iloc[0]\n",
    "\n",
    "    end_x_mirror_c = run_df[\"x_mirror_c\"].iloc[-1]\n",
    "    end_y_mirror_c = run_df[\"y_mirror_c\"].iloc[-1]\n",
    "\n",
    "    coords = run_df[[\"x\", \"y\"]].values\n",
    "\n",
    "    if coords.shape[0] < 2:\n",
    "        run_length = 0.0\n",
    "    else: \n",
    "        deltas = np.diff(coords, axis=0)\n",
    "        segment_lengths = np.linalg.norm(deltas, axis=1)\n",
    "        run_length = np.sum(segment_lengths)\n",
    "\n",
    "    mean_speed = run_df[\"speed\"].mean()\n",
    "    max_speed = run_df[\"speed\"].max()\n",
    "\n",
    "    dx = end_x_mirror_c - start_x_mirror_c\n",
    "    dy = end_y_mirror_c - start_y_mirror_c\n",
    "\n",
    "    run_angle_rad = np.arctan2(dy, dx)\n",
    "    run_angle_deg = np.degrees(run_angle_rad)\n",
    "\n",
    "    run_forward = dx > 0  # True if run is forward (right side of pitch)\n",
    "    \n",
    "    start_zone = get_zone(start_x_mirror_c, start_y_mirror_c, x_edges, y_edges)\n",
    "    end_zone = get_zone(end_x_mirror_c, end_y_mirror_c, x_edges, y_edges)\n",
    "\n",
    "    # Absolute pitch positions\n",
    "    start_x_abs = run_df[\"x\"].iloc[0]\n",
    "    start_y_abs = run_df[\"y\"].iloc[0]\n",
    "\n",
    "    end_x_abs = run_df[\"x\"].iloc[-1]\n",
    "    end_y_abs = run_df[\"y\"].iloc[-1]\n",
    "\n",
    "    start_zone_abs = get_zone(start_x_abs, start_y_abs, x_edges, y_edges)\n",
    "    end_zone_abs = get_zone(end_x_abs, end_y_abs, x_edges, y_edges)\n",
    "\n",
    "    phase_of_play = run_df[\"phase_of_play\"].iloc[0]\n",
    "    in_possession = run_df[\"in_possession\"].iloc[0]\n",
    "    team_role = run_df[\"team_role\"].iloc[0]\n",
    "    position = run_df[\"position\"].iloc[0]\n",
    "\n",
    "    # Extract ball carrier df\n",
    "    ball_carrier_df = extract_ball_carrier_df_fast(\n",
    "        run_df, players_with_ball_df\n",
    "    )\n",
    "    \n",
    "    # Check overlapping run\n",
    "    overlapping = is_overlapping_run(run_df, ball_carrier_df)\n",
    "    underlapping = is_underlapping_run(run_df, ball_carrier_df)\n",
    "    is_diag = is_diagonal_run(run_df)\n",
    "\n",
    "    zone_records.append({\n",
    "        \"run_id\": run_id,\n",
    "        \"start_zone\": start_zone,\n",
    "        \"end_zone\": end_zone,\n",
    "        \"start_zone_absolute\": start_zone_abs,\n",
    "        \"end_zone_absolute\": end_zone_abs,\n",
    "        \"phase_of_play\": phase_of_play,\n",
    "        \"in_possession\": in_possession,\n",
    "        \"team_role\": team_role,\n",
    "        \"position\": position,\n",
    "        \"run_length_m\": run_length,\n",
    "        \"mean_speed\": mean_speed,\n",
    "        \"max_speed\": max_speed,\n",
    "        \"run_angle_deg\": run_angle_deg,\n",
    "        \"run_forward\": run_forward,\n",
    "        \"tactical_overlap\": overlapping,\n",
    "        \"tactical_underlap\": underlapping,\n",
    "        \"tactical_diagonal\": is_diag\n",
    "    })\n",
    "\n",
    "zones_df = pd.DataFrame(zone_records)\n",
    "print(zones_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equation (5): BÃ©zier basis function\n",
    "def bernstein_poly(p, P, t):\n",
    "    return comb(P - 1, p) * (t**p) * ((1 - t)**(P - 1 - p))\n",
    "\n",
    "# Equation (7): Design matrix for BÃ©zier fitting\n",
    "def bezier_design_matrix(num_points, num_control_points):\n",
    "    t_vals = np.linspace(0, 1, num_points)\n",
    "    X = np.stack([bernstein_poly(p, num_control_points, t_vals) for p in range(num_control_points)], axis=1)\n",
    "    return X  # shape: [num_points, num_control_points]\n",
    "\n",
    "# Equation (6): Fit BÃ©zier curve via least squares\n",
    "def fit_bezier_curve(coords, num_control_points):\n",
    "    \"\"\"\n",
    "    coords: shape [N, 2] â€” sequence of (x, y) points\n",
    "    returns: control_points [P, 2]\n",
    "    \"\"\"\n",
    "    N = coords.shape[0]\n",
    "    X = bezier_design_matrix(N, num_control_points)  # shape [N, P]\n",
    "    \n",
    "    # Solve least squares for x and y separately\n",
    "    theta_x, _, _, _ = np.linalg.lstsq(X, coords[:, 0], rcond=None)\n",
    "    theta_y, _, _, _ = np.linalg.lstsq(X, coords[:, 1], rcond=None)\n",
    "    \n",
    "    control_points = np.stack([theta_x, theta_y], axis=1)  # shape: [P, 2]\n",
    "    return control_points\n",
    "\n",
    "# Equation (4): Evaluate BÃ©zier curve at t using control points Î¸\n",
    "def evaluate_bezier_curve(control_points, num_points=50):\n",
    "    \"\"\"\n",
    "    Returns sampled points along the BÃ©zier curve.\n",
    "    \"\"\"\n",
    "    P = control_points.shape[0]\n",
    "    X = bezier_design_matrix(num_points, P)  # shape [num_points, P]\n",
    "    curve = X @ control_points  # shape: [num_points, 2]\n",
    "    return curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_coords(coords, num_points=50):\n",
    "    from scipy.interpolate import interp1d\n",
    "    if len(coords) < 2:\n",
    "        return np.tile(coords[0], (num_points, 1))  # Edge case\n",
    "    distances = np.cumsum(np.linalg.norm(np.diff(coords, axis=0), axis=1))\n",
    "    distances = np.insert(distances, 0, 0.0)\n",
    "    total_length = distances[-1]\n",
    "    if total_length == 0:\n",
    "        return np.tile(coords[0], (num_points, 1))\n",
    "    normalized_dist = distances / total_length\n",
    "    interp_func = interp1d(normalized_dist, coords, axis=0, kind='linear')\n",
    "    uniform_dist = np.linspace(0, 1, num_points)\n",
    "    return interp_func(uniform_dist)\n",
    "\n",
    "def compute_l1_distance(traj, bezier_curve):\n",
    "    \"\"\"\n",
    "    traj, bezier_curve: both of shape [num_points, 2]\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs(traj - bezier_curve))  # L1 averaged over all points and dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Mean objective = 6.4217\n",
      "Iteration 1: Mean objective = 3.6064\n",
      "Iteration 2: Mean objective = 3.0105\n",
      "Iteration 3: Mean objective = 2.8298\n",
      "Iteration 4: Mean objective = 2.7405\n",
      "Iteration 5: Mean objective = 2.6785\n",
      "Iteration 6: Mean objective = 2.6273\n",
      "Iteration 7: Mean objective = 2.5823\n",
      "Iteration 8: Mean objective = 2.5477\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m assigned_cluster \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cluster_idx, control_pts \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cluster_control_points):\n\u001b[0;32m---> 37\u001b[0m     bezier_curve \u001b[38;5;241m=\u001b[39m evaluate_bezier_curve(control_pts, num_points\u001b[38;5;241m=\u001b[39mnum_points)\n\u001b[1;32m     38\u001b[0m     dist \u001b[38;5;241m=\u001b[39m compute_l1_distance(resampled_coords, bezier_curve)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dist \u001b[38;5;241m<\u001b[39m min_dist:\n",
      "Cell \u001b[0;32mIn[20], line 33\u001b[0m, in \u001b[0;36mevaluate_bezier_curve\u001b[0;34m(control_points, num_points)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mReturns sampled points along the BÃ©zier curve.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m P \u001b[38;5;241m=\u001b[39m control_points\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 33\u001b[0m X \u001b[38;5;241m=\u001b[39m bezier_design_matrix(num_points, P)  \u001b[38;5;66;03m# shape [num_points, P]\u001b[39;00m\n\u001b[1;32m     34\u001b[0m curve \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m@\u001b[39m control_points  \u001b[38;5;66;03m# shape: [num_points, 2]\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m curve\n",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m, in \u001b[0;36mbezier_design_matrix\u001b[0;34m(num_points, num_control_points)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbezier_design_matrix\u001b[39m(num_points, num_control_points):\n\u001b[1;32m      7\u001b[0m     t_vals \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, num_points)\n\u001b[0;32m----> 8\u001b[0m     X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([bernstein_poly(p, num_control_points, t_vals) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_control_points)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m, in \u001b[0;36mbernstein_poly\u001b[0;34m(p, P, t)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbernstein_poly\u001b[39m(p, P, t):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m comb(P \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, p) \u001b[38;5;241m*\u001b[39m (t\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mp) \u001b[38;5;241m*\u001b[39m ((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m t)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(P \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m p))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/_lib/deprecation.py:213\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m    216\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[1;32m    219\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/special/_basic.py:2727\u001b[0m, in \u001b[0;36mcomb\u001b[0;34m(N, k, exact, repetition, legacy)\u001b[0m\n\u001b[1;32m   2725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m comb(N, k)\n\u001b[1;32m   2726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2727\u001b[0m     k, N \u001b[38;5;241m=\u001b[39m asarray(k), asarray(N)\n\u001b[1;32m   2728\u001b[0m     cond \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m N) \u001b[38;5;241m&\u001b[39m (N \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (k \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2729\u001b[0m     vals \u001b[38;5;241m=\u001b[39m binom(N, k)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_iterations = 20 # only 1 iteration for now (for debugging)\n",
    "tolerance = 1e-3  # Minimum improvement in objective to continue\n",
    "num_points = 50\n",
    "num_control_points = 4\n",
    "k_clusters = 70\n",
    "\n",
    "# Initialize cluster centers (BÃ©zier curves)\n",
    "random.seed(42)\n",
    "initial_centroids = random.sample(adjusted_runs_list, k_clusters)\n",
    "cluster_control_points = []\n",
    "\n",
    "for run in initial_centroids:\n",
    "    coords = run[[\"x_mirror_c\", \"y_mirror_c\"]].values\n",
    "    control_pts = fit_bezier_curve(coords, num_control_points)\n",
    "    cluster_control_points.append(control_pts)\n",
    "\n",
    "cluster_control_points = np.array(cluster_control_points)\n",
    "previous_objective = float('inf')\n",
    "\n",
    "for it in range(max_iterations):\n",
    "    # Assignment step\n",
    "    assignments = []\n",
    "    objective_distances = []\n",
    "\n",
    "    for run_id, run_df in final_runs_df.groupby(\"run_id\"):\n",
    "        coords = run_df[[\"x_mirror_c\", \"y_mirror_c\"]].values\n",
    "        # resampled_coords = resample_coords(coords, num_points=num_points)\n",
    "\n",
    "        resampled_coords = resample_coords(coords, num_points=num_points)\n",
    "        if resampled_coords is None:\n",
    "            continue  # skip bad run\n",
    "\n",
    "        min_dist = float(\"inf\")\n",
    "        assigned_cluster = -1\n",
    "\n",
    "        for cluster_idx, control_pts in enumerate(cluster_control_points):\n",
    "            bezier_curve = evaluate_bezier_curve(control_pts, num_points=num_points)\n",
    "            dist = compute_l1_distance(resampled_coords, bezier_curve)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                assigned_cluster = cluster_idx\n",
    "\n",
    "        # assignments.append(assigned_cluster)\n",
    "\n",
    "            # Save metadata + cluster assignment\n",
    "        assignments.append({\n",
    "            \"run_id\": run_id,\n",
    "            \"assigned_cluster\": assigned_cluster,\n",
    "            \"min_distance\": min_dist,\n",
    "            \"playerId\": run_df[\"playerId\"].iloc[0],\n",
    "            \"player_name\": run_df[\"player_name\"].iloc[0],\n",
    "            \"position\": run_df[\"position\"].iloc[0],\n",
    "            \"team_role\": run_df[\"team_role\"].iloc[0],\n",
    "            \"match_id\": run_df[\"match_id\"].iloc[0],\n",
    "        })\n",
    "\n",
    "        assignments_df = pd.DataFrame(assignments)\n",
    "\n",
    "        #print(assignments_df.head())\n",
    "\n",
    "        objective_distances.append(min_dist)\n",
    "\n",
    "    objective = np.mean(objective_distances)\n",
    "    print(f\"Iteration {it}: Mean objective = {objective:.4f}\")\n",
    "\n",
    "    # Check for convergence\n",
    "    improvement = previous_objective - objective\n",
    "    if improvement < tolerance:\n",
    "        print(f\"Converged (Î”={improvement:.6f}) at iteration {it}\")\n",
    "        break\n",
    "    previous_objective = objective\n",
    "\n",
    "    # Update step\n",
    "    new_cluster_control_points = []\n",
    "\n",
    "    for cluster_idx in range(k_clusters):\n",
    "        #assigned_indices = [i for i, a in enumerate(assignments) if a == cluster_idx]\n",
    "        assigned_indices = [i for i, a in enumerate(assignments) if a[\"assigned_cluster\"] == cluster_idx]\n",
    "        if not assigned_indices:\n",
    "            new_cluster_control_points.append(cluster_control_points[cluster_idx])\n",
    "            continue\n",
    "\n",
    "        cluster_coords = []\n",
    "        for idx in assigned_indices:\n",
    "            run_id = assignments_df.loc[idx, \"run_id\"]\n",
    "            #run_df = all_runs_df[all_runs_df[\"run_id\"] == run_id]\n",
    "            run_df = final_runs_df[final_runs_df[\"run_id\"] == run_id]\n",
    "\n",
    "            coords = run_df[[\"x_mirror_c\", \"y_mirror_c\"]].values\n",
    "            resampled = resample_coords(coords, num_points=num_points)\n",
    "            # cluster_coords.append(resampled)\n",
    "            if resampled is not None:\n",
    "                cluster_coords.append(resampled)\n",
    "\n",
    "        cluster_coords = np.stack(cluster_coords, axis=0)\n",
    "        mean_coords = np.mean(cluster_coords, axis=0)\n",
    "        new_control_pts = fit_bezier_curve(mean_coords, num_control_points)\n",
    "        new_cluster_control_points.append(new_control_pts)\n",
    "\n",
    "    cluster_control_points = np.array(new_cluster_control_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build assignments DataFrame\n",
    "assignments_df = pd.DataFrame(assignments)\n",
    "\n",
    "assignments_zones = assignments_df.merge(\n",
    "    zones_df,\n",
    "    on=\"run_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Drop duplicates\n",
    "assignments_zones.drop(columns=[\n",
    "    \"position_y\",\n",
    "    \"team_role_y\"\n",
    "], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Rename x columns back to plain names\n",
    "assignments_zones.rename(columns={\n",
    "    \"position_x\": \"position\",\n",
    "    \"team_role_x\": \"team_role\",\n",
    "}, inplace=True)\n",
    "\n",
    "# Count number of runs per position per cluster\n",
    "position_detail_counts = (\n",
    "    assignments_zones\n",
    "    .groupby([\"assigned_cluster\", \"position\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"num_runs\")\n",
    "    .sort_values([\"assigned_cluster\", \"num_runs\"], ascending=[True, False])\n",
    ")\n",
    "\n",
    "position_pivot = (\n",
    "    position_detail_counts\n",
    "    .pivot_table(index=\"assigned_cluster\",\n",
    "                 columns=\"position\",\n",
    "                 values=\"num_runs\",\n",
    "                 fill_value=0)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep first row of each run\n",
    "runs_meta_df = final_runs_df.groupby(\"run_id\", as_index=False).first()\n",
    "\n",
    "# Merge metadata into assignments\n",
    "merged_df = assignments_df.merge(\n",
    "    runs_meta_df,\n",
    "    on=\"run_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Clean up columns\n",
    "merged_df.drop(columns=[\n",
    "    \"position_y\",\n",
    "    \"player_name_y\",\n",
    "    \"team_role_y\",\n",
    "    \"match_id_y\",\n",
    "    \"playerId_y\",\n",
    "], inplace=True, errors=\"ignore\")\n",
    "\n",
    "merged_df.rename(columns={\n",
    "    \"player_name_x\": \"player_name\",\n",
    "    \"position_x\": \"position\",\n",
    "    \"team_role_x\": \"team_role\",\n",
    "    \"match_id_x\": \"match_id\",\n",
    "    \"playerId_x\": \"playerId\",\n",
    "}, inplace=True)\n",
    "\n",
    "#print(\"merged_df columns:\", merged_df.columns)\n",
    "\n",
    "# Group and count\n",
    "position_counts = (\n",
    "    merged_df\n",
    "    .groupby([\"assigned_cluster\", \"position\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"num_runs\")\n",
    "    .sort_values([\"assigned_cluster\", \"num_runs\"], ascending=[True, False])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of fine-grained positions â†’ high-level buckets\n",
    "position_bucket_map = {\n",
    "    \"GK\": \"sub\",       # treat as non-field player for running\n",
    "    \"SUB\": \"sub\",\n",
    "    \n",
    "    # Defenders\n",
    "    \"CB\": \"defender\",\n",
    "    \"RCB\": \"defender\",\n",
    "    \"LCB\": \"defender\",\n",
    "    \"RB\": \"defender\",\n",
    "    \"LB\": \"defender\",\n",
    "    \"RWB\": \"defender\",\n",
    "    \"LWB\": \"defender\",\n",
    "    \n",
    "    # Midfielders\n",
    "    \"CDM\": \"midfielder\",\n",
    "    \"RDM\": \"midfielder\",\n",
    "    \"LDM\": \"midfielder\",\n",
    "    \"CM\": \"midfielder\",\n",
    "    \"RCM\": \"midfielder\",\n",
    "    \"LCM\": \"midfielder\",\n",
    "    \"CAM\": \"midfielder\",\n",
    "    \"RM\": \"midfielder\",\n",
    "    \"LM\": \"midfielder\",\n",
    "    \n",
    "    # Attackers\n",
    "    \"LW\": \"attacker\",\n",
    "    \"RW\": \"attacker\",\n",
    "    \"ST\": \"attacker\",\n",
    "    \"CF\": \"attacker\",\n",
    "    \"RF\": \"attacker\",\n",
    "    \"LF\": \"attacker\",\n",
    "}\n",
    "\n",
    "# Map positions to buckets\n",
    "merged_df[\"position_bucket\"] = merged_df[\"position\"].map(\n",
    "    lambda pos: position_bucket_map.get(pos, \"unknown\")\n",
    ")\n",
    "\n",
    "print(merged_df[[\"assigned_cluster\", \"position\", \"position_bucket\"]].head(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_counts = (\n",
    "    merged_df\n",
    "    .groupby([\"assigned_cluster\", \"position_bucket\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"num_runs\")\n",
    "    .sort_values([\"assigned_cluster\", \"num_runs\"], ascending=[True, False])\n",
    ")\n",
    "\n",
    "\n",
    "bucket_pivot = (\n",
    "    bucket_counts\n",
    "    .pivot_table(index=\"assigned_cluster\", \n",
    "                 columns=\"position_bucket\", \n",
    "                 values=\"num_runs\", \n",
    "                 fill_value=0)\n",
    "    .reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pitch(ax, pitch_length=105, pitch_width=68):\n",
    "    \"\"\"\n",
    "    Draws a football pitch centered around (0, 0) on the given matplotlib axis.\n",
    "    \"\"\"\n",
    "    half_length = pitch_length / 2\n",
    "    half_width = pitch_width / 2\n",
    "\n",
    "    # Outer boundary & centre line\n",
    "    ax.plot([-half_length, -half_length, half_length, half_length, -half_length],\n",
    "            [-half_width, half_width, half_width, -half_width, -half_width], color=\"black\")\n",
    "    ax.plot([0, 0], [-half_width, half_width], color=\"black\")\n",
    "\n",
    "    # Left penalty area\n",
    "    ax.plot([-half_length + 16.5, -half_length + 16.5], [-13.84, 13.84], color=\"black\")\n",
    "    ax.plot([-half_length, -half_length + 16.5], [-13.84, -13.84], color=\"black\")\n",
    "    ax.plot([-half_length, -half_length + 16.5], [13.84, 13.84], color=\"black\")\n",
    "\n",
    "    # Right penalty area\n",
    "    ax.plot([half_length - 16.5, half_length - 16.5], [-13.84, 13.84], color=\"black\")\n",
    "    ax.plot([half_length, half_length - 16.5], [-13.84, -13.84], color=\"black\")\n",
    "    ax.plot([half_length, half_length - 16.5], [13.84, 13.84], color=\"black\")\n",
    "\n",
    "    # Center circle\n",
    "    circle = plt.Circle((0, 0), 9.15, color=\"black\", fill=False)\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "    ax.set_xlim(-half_length, half_length)\n",
    "    ax.set_ylim(-half_width, half_width)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_cluster_trajectories_on_pitch(\n",
    "    final_runs_df,\n",
    "    assignments_zones,\n",
    "    cluster_control_points,\n",
    "    bucket_pivot,\n",
    "    num_control_points=4,\n",
    "    max_runs_per_cluster=30,\n",
    "    plot_absolute_positions=True,\n",
    "    start_zones=None,\n",
    "    end_zones=None,\n",
    "    phases_of_play=None,\n",
    "    positions=None,\n",
    "    use_absolute_zones=False,\n",
    "    start_zones_absolute=None,\n",
    "    end_zones_absolute=None,\n",
    "    run_angle_range = None, #tuple of (min_angle, max_angle) in degrees\n",
    "    run_forward = None, #bool, whether to filter for forward runs only\n",
    "    run_length_range = None, #tuple of (min_length, max_length) in meters\n",
    "    mean_speed_range = None, #tuple of (min_speed, max_speed) in meters per second\n",
    "    max_speed_range = None, #tuple of (min_speed, max_speed)\n",
    "    tactical_overlap=None,\n",
    "    tactical_underlap=None,\n",
    "    tactical_diagonal=None\n",
    "):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    num_clusters = len(cluster_control_points)\n",
    "    fig, axes = plt.subplots(7, 10, figsize=(30, 20))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # --- FILTERING ---\n",
    "    filtered_assignments = assignments_zones.copy()\n",
    "\n",
    "    if start_zones is not None and not use_absolute_zones:\n",
    "        filtered_assignments = filtered_assignments[\n",
    "            filtered_assignments[\"start_zone\"].isin(start_zones)\n",
    "        ]\n",
    "\n",
    "    if end_zones is not None and not use_absolute_zones:\n",
    "        filtered_assignments = filtered_assignments[\n",
    "            filtered_assignments[\"end_zone\"].isin(end_zones)\n",
    "        ]\n",
    "\n",
    "    if start_zones_absolute is not None and use_absolute_zones:\n",
    "        filtered_assignments = filtered_assignments[\n",
    "            filtered_assignments[\"start_zone_absolute\"].isin(start_zones_absolute)\n",
    "        ]\n",
    "\n",
    "    if end_zones_absolute is not None and use_absolute_zones:\n",
    "        filtered_assignments = filtered_assignments[\n",
    "            filtered_assignments[\"end_zone_absolute\"].isin(end_zones_absolute)\n",
    "        ]\n",
    "\n",
    "    if phases_of_play is not None:\n",
    "        filtered_assignments = filtered_assignments[\n",
    "            filtered_assignments[\"phase_of_play\"].isin(phases_of_play)\n",
    "        ]\n",
    "\n",
    "    if positions is not None:\n",
    "        filtered_assignments = filtered_assignments[\n",
    "            filtered_assignments[\"position\"].isin(positions)\n",
    "        ]\n",
    "\n",
    "    if run_angle_range is not None:\n",
    "        min_angle, max_angle = run_angle_range\n",
    "        filtered_assignments = filtered_assignments[\n",
    "            (filtered_assignments[\"run_angle_deg\"] >= min_angle) &\n",
    "            (filtered_assignments[\"run_angle_deg\"] <= max_angle)]\n",
    "    \n",
    "    if run_forward is not None: \n",
    "        filtered_assignments = filtered_assignments[\n",
    "            filtered_assignments[\"run_forward\"] == run_forward]\n",
    "    \n",
    "    if run_length_range is not None:\n",
    "        min_len, max_len = run_length_range\n",
    "        filtered_assignments = filtered_assignments[\n",
    "            (filtered_assignments[\"run_length_m\"] >= min_len) &\n",
    "            (filtered_assignments[\"run_length_m\"] <= max_len)\n",
    "        ]\n",
    "\n",
    "    if mean_speed_range is not None:\n",
    "        min_speed, max_speed = mean_speed_range\n",
    "        filtered_assignments = filtered_assignments[\n",
    "            (filtered_assignments[\"mean_speed\"] >= min_speed) &\n",
    "            (filtered_assignments[\"mean_speed\"] <= max_speed)\n",
    "        ]\n",
    "\n",
    "    if max_speed_range is not None:\n",
    "        min_speed, max_speed = max_speed_range\n",
    "        filtered_assignments = filtered_assignments[\n",
    "            (filtered_assignments[\"max_speed\"] >= min_speed) &\n",
    "            (filtered_assignments[\"max_speed\"] <= max_speed)\n",
    "        ]\n",
    "\n",
    "    if tactical_overlap is not None:\n",
    "        filtered_assignments = filtered_assignments[\n",
    "            filtered_assignments[\"tactical_overlap\"] == tactical_overlap\n",
    "        ]\n",
    "\n",
    "    if tactical_underlap is not None:\n",
    "        filtered_assignments = filtered_assignments[\n",
    "            filtered_assignments[\"tactical_underlap\"] == tactical_underlap\n",
    "        ]\n",
    "\n",
    "    if tactical_diagonal is not None:\n",
    "        filtered_assignments = filtered_assignments[\n",
    "            filtered_assignments[\"tactical_diagonal\"] == tactical_diagonal\n",
    "        ]\n",
    "\n",
    "    filtered_run_ids = filtered_assignments[\"run_id\"].unique()\n",
    "\n",
    "    bucket_pivot = bucket_pivot.set_index(\"assigned_cluster\")\n",
    "\n",
    "    for cluster_idx in range(num_clusters):\n",
    "        ax = axes[cluster_idx]\n",
    "        draw_pitch(ax)\n",
    "\n",
    "        cluster_run_ids = assignments_zones.loc[\n",
    "            assignments_zones[\"assigned_cluster\"] == cluster_idx, \"run_id\"\n",
    "        ].tolist()\n",
    "\n",
    "        cluster_run_ids = [\n",
    "            rid for rid in cluster_run_ids if rid in filtered_run_ids\n",
    "        ]\n",
    "\n",
    "        if len(cluster_run_ids) > max_runs_per_cluster:\n",
    "            cluster_run_ids = random.sample(cluster_run_ids, max_runs_per_cluster)\n",
    "\n",
    "        for run_id in cluster_run_ids:\n",
    "            run_df = final_runs_df[final_runs_df[\"run_id\"] == run_id]\n",
    "\n",
    "            coords = run_df[[\"x_mirror_c\", \"y_mirror_c\"]].values\n",
    "            if coords.shape[0] < 2:\n",
    "                continue\n",
    "\n",
    "            resampled = resample_coords(coords, num_points=50)\n",
    "            control_pts = fit_bezier_curve(resampled, num_control_points)\n",
    "\n",
    "            if plot_absolute_positions:\n",
    "                start_pos = run_df[[\"x\", \"y\"]].values[0]\n",
    "                shifted_ctrl_pts = control_pts - control_pts[0] + start_pos\n",
    "                bezier_curve = evaluate_bezier_curve(\n",
    "                    shifted_ctrl_pts, num_points=50\n",
    "                )\n",
    "            else:\n",
    "                bezier_curve = evaluate_bezier_curve(\n",
    "                    control_pts, num_points=50\n",
    "                )\n",
    "\n",
    "            ax.plot(bezier_curve[:, 0], bezier_curve[:, 1], alpha=0.2, color=\"blue\")\n",
    "\n",
    "        if cluster_idx in bucket_pivot.index:\n",
    "            row = bucket_pivot.loc[cluster_idx]\n",
    "            text_lines = []\n",
    "            for bucket in [\"attacker\", \"midfielder\", \"defender\", \"sub\", \"unknown\"]:\n",
    "                if bucket in row and row[bucket] > 0:\n",
    "                    text_lines.append(f\"{bucket}: {int(row[bucket])}\")\n",
    "            text = \"\\n\".join(text_lines)\n",
    "            ax.text(\n",
    "                0.5, 1.2, text,\n",
    "                transform=ax.transAxes,\n",
    "                ha=\"center\", va=\"bottom\",\n",
    "                fontsize=8,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8)\n",
    "            )\n",
    "\n",
    "        ax.set_title(f\"Cluster {cluster_idx}\", fontsize=8)\n",
    "\n",
    "    for i in range(num_clusters, len(axes)):\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"All BÃ©zier Run Trajectories per Cluster (On-Pitch View)\", fontsize=16, y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "# Define edges matching your pitch grid\n",
    "x_edges = np.linspace(-52.5, 52.5, 4)\n",
    "y_edges = np.linspace(-34, 34, 4)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Draw pitch outline\n",
    "draw_pitch(ax)\n",
    "\n",
    "zone_idx = 1\n",
    "\n",
    "# Draw zone rectangles and labels\n",
    "for y_bin in range(3):\n",
    "    for x_bin in range(3):\n",
    "        x_left = x_edges[x_bin]\n",
    "        x_right = x_edges[x_bin + 1]\n",
    "        y_bottom = y_edges[y_bin]\n",
    "        y_top = y_edges[y_bin + 1]\n",
    "\n",
    "        width = x_right - x_left\n",
    "        height = y_top - y_bottom\n",
    "\n",
    "        rect = patches.Rectangle(\n",
    "            (x_left, y_bottom),\n",
    "            width,\n",
    "            height,\n",
    "            linewidth=1,\n",
    "            edgecolor='black',\n",
    "            facecolor='lightgrey',\n",
    "            alpha=0.2\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Label in center\n",
    "        x_center = (x_left + x_right) / 2\n",
    "        y_center = (y_bottom + y_top) / 2\n",
    "\n",
    "        ax.text(\n",
    "            x_center, y_center,\n",
    "            str(zone_idx),\n",
    "            ha='center',\n",
    "            va='center',\n",
    "            fontsize=14,\n",
    "            fontweight='bold',\n",
    "            color='black'\n",
    "        )\n",
    "\n",
    "        zone_idx += 1\n",
    "\n",
    "ax.set_title(\"Zone Legend\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_cluster_trajectories_on_pitch(\n",
    "    final_runs_df,\n",
    "    assignments_zones,\n",
    "    cluster_control_points,\n",
    "    bucket_pivot=bucket_pivot,\n",
    "    num_control_points=4,\n",
    "    max_runs_per_cluster=200,\n",
    "    plot_absolute_positions=False,\n",
    "    start_zones=None,\n",
    "    end_zones=None,  # You can specify zones like [1, 2, 3] if you want to filter\n",
    "    phases_of_play=None,\n",
    "    positions=None,\n",
    "    use_absolute_zones=False, # True or False are options\n",
    "    start_zones_absolute=None,\n",
    "    end_zones_absolute=None,\n",
    "    run_angle_range=None,  # Example: filter for runs with angles between 0 and 180 degrees\n",
    "    run_forward=True,\n",
    "    run_length_range = None,  # Example: filter for runs with length between 0 and 100 meters\n",
    "    mean_speed_range = None,  # Example: filter for runs with mean speed between 0 and 8 m/s\n",
    "    max_speed_range = None,\n",
    "    tactical_overlap=None,\n",
    "    tactical_underlap=None,\n",
    "    tactical_diagonal=None     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ball_carrier_df_fast(run_df, players_with_ball_df):\n",
    "    \"\"\"\n",
    "    Fast version of extract_ball_carrier_df.\n",
    "    \"\"\"\n",
    "    match_id = run_df[\"match_id\"].iloc[0]\n",
    "    period = run_df[\"period\"].iloc[0]\n",
    "    team_role = run_df[\"team_role\"].iloc[0]\n",
    "    frames = run_df[\"frameIdx\"].unique()\n",
    "\n",
    "    # Slice relevant frames for this run\n",
    "    run_players = players_with_ball_df[\n",
    "        (players_with_ball_df[\"match_id\"] == match_id) &\n",
    "        (players_with_ball_df[\"period\"] == period) &\n",
    "        (players_with_ball_df[\"frameIdx\"].isin(frames))\n",
    "    ]\n",
    "\n",
    "    # Keep only teammates\n",
    "    teammates = run_players[\n",
    "        run_players[\"side\"] == team_role\n",
    "    ].copy()\n",
    "\n",
    "    if teammates.empty:\n",
    "        return None\n",
    "\n",
    "    # Compute distance to ball\n",
    "    teammates[\"dist_to_ball\"] = np.linalg.norm(\n",
    "        teammates[[\"x\", \"y\"]].values - teammates[[\"ball_x\", \"ball_y\"]].values,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Find closest player in each frame\n",
    "    idx_min_dist = teammates.groupby(\"frameIdx\")[\"dist_to_ball\"].idxmin()\n",
    "\n",
    "    ball_carrier_df = teammates.loc[idx_min_dist]\n",
    "\n",
    "    if ball_carrier_df.empty:\n",
    "        return None\n",
    "    else:\n",
    "        return ball_carrier_df\n",
    "    \n",
    "# def is_overlapping_run(run_df, events_df, time_margin_seconds=1.0,\n",
    "#                        max_angle_diff_deg=30, max_end_distance=5.0,\n",
    "#                        min_pass_length=5.0,\n",
    "#                        exclude_pass_types=None,\n",
    "#                        require_regular_play=True):\n",
    "#     \"\"\"\n",
    "#     Determines whether a run overlaps a pass in the event data.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     run_df : pd.DataFrame\n",
    "#         Tracking run data (single run).\n",
    "#     events_df : pd.DataFrame\n",
    "#         StatsBomb events dataframe for the same match.\n",
    "#     time_margin_seconds : float\n",
    "#         Allowable margin before and after run times.\n",
    "#     max_angle_diff_deg : float\n",
    "#         Max angle difference between run vector and pass vector.\n",
    "#     max_end_distance : float\n",
    "#         Max allowed distance between run end point and pass target.\n",
    "#     min_pass_length : float\n",
    "#         Minimum length of pass to consider overlaps.\n",
    "#     exclude_pass_types : list of str\n",
    "#         E.g. [\"Kick Off\", \"Throw-In\"]\n",
    "#     require_regular_play : bool\n",
    "#         If True, only consider passes in Regular Play.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     bool\n",
    "#         True if run matches an overlapping run pattern.\n",
    "#     \"\"\"\n",
    "\n",
    "#     if exclude_pass_types is None:\n",
    "#         exclude_pass_types = [\n",
    "#             \"Kick Off\", \"Throw-In\", \"Goal Kick\", \"Free Kick\", \"Corner\"\n",
    "#         ]\n",
    "\n",
    "#     # -------------------------------------------------------\n",
    "#     # Step 1: Compute run timing\n",
    "#     # -------------------------------------------------------\n",
    "#     match_id = run_df[\"match_id\"].iloc[0]\n",
    "#     period = run_df[\"period\"].iloc[0]\n",
    "\n",
    "#     run_start_frame = run_df[\"frameIdx\"].iloc[0]\n",
    "#     run_end_frame = run_df[\"frameIdx\"].iloc[-1]\n",
    "\n",
    "#     # Convert frame index to time if needed\n",
    "#     # (assuming you have frames_df available globally)\n",
    "#     # Otherwise approximate:\n",
    "#     frame_rate = 25.0  # or your data's FPS\n",
    "#     run_start_time = run_start_frame / frame_rate\n",
    "#     run_end_time = run_end_frame / frame_rate\n",
    "\n",
    "#     # -------------------------------------------------------\n",
    "#     # Step 2: Subset events from same period\n",
    "#     # -------------------------------------------------------\n",
    "#     period_events = events_df[\n",
    "#         (events_df[\"period\"] == period) &\n",
    "#         (events_df[\"type.name\"] == \"Pass\")\n",
    "#     ].copy()\n",
    "\n",
    "#     if period_events.empty:\n",
    "#         return False\n",
    "\n",
    "#     # -------------------------------------------------------\n",
    "#     # Step 3: Iterate passes and check conditions\n",
    "#     # -------------------------------------------------------\n",
    "#     for _, pass_row in period_events.iterrows():\n",
    "#         # Time of this pass\n",
    "#         pass_minute = pass_row[\"minute\"]\n",
    "#         pass_second = pass_row[\"second\"]\n",
    "#         pass_time_seconds = pass_minute * 60 + pass_second\n",
    "\n",
    "#         # Check run overlaps pass time\n",
    "#         if not (run_start_time - time_margin_seconds <= pass_time_seconds <= run_end_time + time_margin_seconds):\n",
    "#             continue\n",
    "\n",
    "#         # Check same team\n",
    "#         run_team = run_df[\"team_role\"].iloc[0]\n",
    "#         pass_team_name = pass_row.get(\"team.name\", None)\n",
    "#         if pass_team_name is None:\n",
    "#             continue\n",
    "\n",
    "#         # Optional: check possession id\n",
    "#         run_possession = run_df.get(\"possession\", [None])[0]\n",
    "#         pass_possession = pass_row.get(\"possession\", None)\n",
    "#         if run_possession is not None and pass_possession is not None:\n",
    "#             if run_possession != pass_possession:\n",
    "#                 continue\n",
    "\n",
    "#         # Optional: check play pattern\n",
    "#         if require_regular_play:\n",
    "#             play_pattern = pass_row.get(\"play_pattern.name\", None)\n",
    "#             if play_pattern and play_pattern != \"Regular Play\":\n",
    "#                 continue\n",
    "\n",
    "#         # Exclude certain pass types\n",
    "#         pass_type = pass_row.get(\"pass.type.name\", \"\")\n",
    "#         if pass_type in exclude_pass_types:\n",
    "#             continue\n",
    "\n",
    "#         # Minimum length\n",
    "#         pass_length = pass_row.get(\"pass.length\", 0.0)\n",
    "#         if pass_length < min_pass_length:\n",
    "#             continue\n",
    "\n",
    "#         # Check carrier == passer\n",
    "#         carrier_ids = run_df[\"playerId\"].unique()\n",
    "#         passer_id = pass_row.get(\"player.id\", None)\n",
    "#         if passer_id is None or passer_id not in carrier_ids:\n",
    "#             continue\n",
    "\n",
    "#         # Check angle alignment\n",
    "#         run_start = run_df.iloc[0][[\"x\", \"y\"]].values\n",
    "#         run_end = run_df.iloc[-1][[\"x\", \"y\"]].values\n",
    "#         run_vector = run_end - run_start\n",
    "\n",
    "#         run_angle = np.arctan2(run_vector[1], run_vector[0])\n",
    "#         run_angle_deg = np.degrees(run_angle)\n",
    "\n",
    "#         pass_angle_rad = pass_row.get(\"pass.angle\", None)\n",
    "#         if pass_angle_rad is not None:\n",
    "#             angle_diff = abs(np.degrees(pass_angle_rad) - run_angle_deg)\n",
    "#             angle_diff = angle_diff % 360\n",
    "#             if angle_diff > 180:\n",
    "#                 angle_diff = 360 - angle_diff\n",
    "\n",
    "#             if angle_diff > max_angle_diff_deg:\n",
    "#                 continue\n",
    "\n",
    "#         # Check run end near pass end location\n",
    "#         pass_end_loc = pass_row.get(\"pass.end_location\", None)\n",
    "#         if pass_end_loc is not None:\n",
    "#             run_end_xy = run_end\n",
    "#             pass_end_xy = np.array(pass_end_loc)\n",
    "#             dist = np.linalg.norm(run_end_xy - pass_end_xy)\n",
    "#             if dist > max_end_distance:\n",
    "#                 continue\n",
    "\n",
    "#         # If we reached here â†’ overlapping run\n",
    "#         return True\n",
    "\n",
    "#     # No matching pass found\n",
    "#     return False\n",
    "\n",
    "def is_overlapping_run_simple(\n",
    "    run_df,\n",
    "    events_df,\n",
    "    time_margin_seconds=1.0,\n",
    "    max_angle_diff_deg=30,\n",
    "):\n",
    "    \"\"\"\n",
    "    Simple heuristic for overlapping runs:\n",
    "    - Pass and run overlap in time\n",
    "    - Same team\n",
    "    - Same player as passer\n",
    "    - Run and pass go roughly same direction\n",
    "    \"\"\"\n",
    "    if events_df is None or events_df.empty:\n",
    "        return False\n",
    "\n",
    "    match_id = run_df[\"match_id\"].iloc[0]\n",
    "    period = run_df[\"period\"].iloc[0]\n",
    "\n",
    "    # Compute run timing (seconds)\n",
    "    frame_rate = 25.0\n",
    "    run_start_frame = run_df[\"frameIdx\"].iloc[0]\n",
    "    run_end_frame = run_df[\"frameIdx\"].iloc[-1]\n",
    "    run_start_time = run_start_frame / frame_rate\n",
    "    run_end_time = run_end_frame / frame_rate\n",
    "\n",
    "    # Select only pass events in same period\n",
    "    period_passes = events_df[\n",
    "        (events_df[\"period\"] == period) &\n",
    "        (events_df[\"type.name\"] == \"Pass\")\n",
    "    ].copy()\n",
    "\n",
    "    if period_passes.empty:\n",
    "        return False\n",
    "\n",
    "    # Compute run vector\n",
    "    run_start = run_df.iloc[0][[\"x\", \"y\"]].values\n",
    "    run_end = run_df.iloc[-1][[\"x\", \"y\"]].values\n",
    "    run_vector = run_end - run_start\n",
    "    run_angle_deg = np.degrees(np.arctan2(run_vector[1], run_vector[0]))\n",
    "\n",
    "    runner_ids = run_df[\"playerId\"].unique()\n",
    "    run_team = run_df[\"team_role\"].iloc[0]\n",
    "\n",
    "    for _, pass_row in period_passes.iterrows():\n",
    "        # Time of the pass\n",
    "        pass_time = pass_row[\"minute\"] * 60 + pass_row[\"second\"]\n",
    "\n",
    "        if not (run_start_time - time_margin_seconds <= pass_time <= run_end_time + time_margin_seconds):\n",
    "            continue\n",
    "\n",
    "        # Team check\n",
    "        if pass_row.get(\"team.name\") != run_team:\n",
    "            continue\n",
    "\n",
    "        # Player check\n",
    "        passer_id = pass_row.get(\"player.id\", None)\n",
    "        if passer_id is None or passer_id not in runner_ids:\n",
    "            continue\n",
    "\n",
    "        # Direction check\n",
    "        pass_angle_rad = pass_row.get(\"pass.angle\", None)\n",
    "        if pass_angle_rad is None:\n",
    "            continue\n",
    "\n",
    "        pass_angle_deg = np.degrees(pass_angle_rad)\n",
    "        angle_diff = abs(pass_angle_deg - run_angle_deg) % 360\n",
    "        if angle_diff > 180:\n",
    "            angle_diff = 360 - angle_diff\n",
    "\n",
    "        if angle_diff > max_angle_diff_deg:\n",
    "            continue\n",
    "\n",
    "        # If we reach here â†’ overlapping run\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_underlapping_run(run_df, \n",
    "                        ball_carrier_df, \n",
    "                        events_df, \n",
    "                        time_window_sec=2.0, \n",
    "                        min_pass_distance=5.0,\n",
    "                        max_angle_diff_deg=45):\n",
    "    \"\"\"\n",
    "    Heuristic + event-based detection of underlapping runs.\n",
    "    \n",
    "    - Checks geometry (runner cuts inside)\n",
    "    - Checks for matching pass or carry event\n",
    "    \n",
    "    Returns True only if:\n",
    "      - ball carrier remains constant during run\n",
    "      - a pass or carry event matches the run\n",
    "    \"\"\"\n",
    "\n",
    "    if ball_carrier_df is None:\n",
    "        return False\n",
    "\n",
    "    # Check if ball carrier changed\n",
    "    unique_carriers = ball_carrier_df[\"playerId\"].nunique()\n",
    "    if unique_carriers > 1:\n",
    "        return False\n",
    "\n",
    "    f_start = run_df[\"frameIdx\"].iloc[0]\n",
    "    f_end = run_df[\"frameIdx\"].iloc[-1]\n",
    "    start_time = run_df[\"gameClock\"].iloc[0]\n",
    "    end_time = run_df[\"gameClock\"].iloc[-1]\n",
    "\n",
    "    runner_id = run_df[\"playerId\"].iloc[0]\n",
    "    carrier_id = ball_carrier_df[\"playerId\"].iloc[0]\n",
    "\n",
    "    runner_start = run_df.iloc[0][[\"x\", \"y\"]].values\n",
    "    runner_end = run_df.iloc[-1][[\"x\", \"y\"]].values\n",
    "\n",
    "    carrier_start_row = ball_carrier_df.loc[\n",
    "        ball_carrier_df[\"frameIdx\"] == f_start\n",
    "    ]\n",
    "    carrier_end_row = ball_carrier_df.loc[\n",
    "        ball_carrier_df[\"frameIdx\"] == f_end\n",
    "    ]\n",
    "\n",
    "    if carrier_start_row.empty or carrier_end_row.empty:\n",
    "        return False\n",
    "\n",
    "    carrier_start = carrier_start_row.iloc[0][[\"x\", \"y\"]].values\n",
    "    carrier_end = carrier_end_row.iloc[0][[\"x\", \"y\"]].values\n",
    "\n",
    "    # --- Geometric checks ---\n",
    "\n",
    "    # Lateral offset (y difference)\n",
    "    delta_start_y = runner_start[1] - carrier_start[1]\n",
    "    delta_end_y = runner_end[1] - carrier_end[1]\n",
    "\n",
    "    overlap_side_change = np.sign(delta_start_y) != np.sign(delta_end_y)\n",
    "\n",
    "    lateral_distance_start = abs(delta_start_y)\n",
    "    lateral_distance_end = abs(delta_end_y)\n",
    "\n",
    "    lateral_movement = lateral_distance_start - lateral_distance_end\n",
    "\n",
    "    forward_distance = runner_end[0] - runner_start[0]\n",
    "\n",
    "    geometry_check = (\n",
    "        overlap_side_change and\n",
    "        lateral_movement > 1.0 and\n",
    "        forward_distance > min_pass_distance\n",
    "    )\n",
    "\n",
    "    if not geometry_check:\n",
    "        return False\n",
    "\n",
    "    # --- Event-based checks ---\n",
    "\n",
    "    # Find candidate events shortly after the run\n",
    "    period = run_df[\"period\"].iloc[0]\n",
    "\n",
    "    candidate_events = events_df[\n",
    "        (events_df[\"period\"] == period) &\n",
    "        (events_df[\"second\"] + events_df[\"minute\"]*60 >= end_time) &\n",
    "        (events_df[\"second\"] + events_df[\"minute\"]*60 <= end_time + time_window_sec)\n",
    "    ]\n",
    "\n",
    "    # Check for passes from carrier to runner\n",
    "    pass_events = candidate_events[\n",
    "        (candidate_events[\"type\"].apply(lambda d: d.get(\"name\") if isinstance(d, dict) else None) == \"Pass\")\n",
    "    ]\n",
    "\n",
    "    for _, ev in pass_events.iterrows():\n",
    "        ev_player = ev.get(\"player\", {})\n",
    "        ev_pass = ev.get(\"pass\", {})\n",
    "        ev_recipient = ev_pass.get(\"recipient\", {})\n",
    "        \n",
    "        if ev_player and ev_player.get(\"id\") == carrier_id:\n",
    "            if ev_recipient and ev_recipient.get(\"id\") == runner_id:\n",
    "                # Compare run vector vs pass vector\n",
    "                ev_start_loc = np.array(ev[\"location\"]) if ev.get(\"location\") else None\n",
    "                ev_end_loc = np.array(ev_pass.get(\"end_location\")) if ev_pass.get(\"end_location\") else None\n",
    "\n",
    "                if ev_start_loc is not None and ev_end_loc is not None:\n",
    "                    pass_vector = ev_end_loc - ev_start_loc\n",
    "                    run_vector = runner_end - runner_start\n",
    "\n",
    "                    # Compute angle between vectors\n",
    "                    cos_sim = np.dot(pass_vector, run_vector) / (\n",
    "                        np.linalg.norm(pass_vector) * np.linalg.norm(run_vector) + 1e-6\n",
    "                    )\n",
    "                    angle_deg = np.degrees(np.arccos(np.clip(cos_sim, -1.0, 1.0)))\n",
    "\n",
    "                    if angle_deg < max_angle_diff_deg:\n",
    "                        return True\n",
    "\n",
    "    # Optionally, check for carries:\n",
    "    carry_events = candidate_events[\n",
    "        (candidate_events[\"type\"].apply(lambda d: d.get(\"name\") if isinstance(d, dict) else None) == \"Carry\")\n",
    "    ]\n",
    "\n",
    "    for _, ev in carry_events.iterrows():\n",
    "        ev_player = ev.get(\"player\", {})\n",
    "        ev_carry = ev.get(\"carry\", {})\n",
    "\n",
    "        if ev_player and ev_player.get(\"id\") == carrier_id:\n",
    "            ev_end_loc = np.array(ev_carry.get(\"end_location\")) if ev_carry else None\n",
    "\n",
    "            if ev_end_loc is not None:\n",
    "                run_vector = runner_end - runner_start\n",
    "                carry_vector = ev_end_loc - carrier_end\n",
    "\n",
    "                cos_sim = np.dot(carry_vector, run_vector) / (\n",
    "                    np.linalg.norm(carry_vector) * np.linalg.norm(run_vector) + 1e-6\n",
    "                )\n",
    "                angle_deg = np.degrees(np.arccos(np.clip(cos_sim, -1.0, 1.0)))\n",
    "\n",
    "                if angle_deg < max_angle_diff_deg:\n",
    "                    return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid\n",
    "x_edges = np.linspace(-52.5, 52.5, 4)   # splits pitch length into thirds\n",
    "y_edges = np.linspace(-34, 34, 4)       # splits width into thirds\n",
    "\n",
    "def get_zone(x, y, x_edges, y_edges):\n",
    "    x_bin = np.digitize([x], x_edges)[0] - 1\n",
    "    y_bin = np.digitize([y], y_edges)[0] - 1\n",
    "    x_bin = min(max(x_bin, 0), len(x_edges)-2)\n",
    "    y_bin = min(max(y_bin, 0), len(y_edges)-2)\n",
    "    zone_idx = y_bin * (len(x_edges)-1) + x_bin + 1\n",
    "    return zone_idx\n",
    "\n",
    "# Build players_with_ball_df ONCE for all runs\n",
    "players_with_ball_df = players_df.merge(\n",
    "    frames_df[[\"match_id\", \"period\", \"frameIdx\", \"ball_x\", \"ball_y\", \"lastTouch_team\"]],\n",
    "    on=[\"match_id\", \"period\", \"frameIdx\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_ball\")\n",
    ")\n",
    "\n",
    "zone_records = []\n",
    "\n",
    "for run_id, run_df in final_runs_df.groupby(\"run_id\"):\n",
    "\n",
    "    # Mirrored & centered positions\n",
    "    start_x_mirror_c = run_df[\"x_mirror_c\"].iloc[0]\n",
    "    start_y_mirror_c = run_df[\"y_mirror_c\"].iloc[0]\n",
    "\n",
    "    end_x_mirror_c = run_df[\"x_mirror_c\"].iloc[-1]\n",
    "    end_y_mirror_c = run_df[\"y_mirror_c\"].iloc[-1]\n",
    "\n",
    "    coords = run_df[[\"x\", \"y\"]].values\n",
    "\n",
    "    if coords.shape[0] < 2:\n",
    "        run_length = 0.0\n",
    "    else: \n",
    "        deltas = np.diff(coords, axis=0)\n",
    "        segment_lengths = np.linalg.norm(deltas, axis=1)\n",
    "        run_length = np.sum(segment_lengths)\n",
    "\n",
    "    mean_speed = run_df[\"speed\"].mean()\n",
    "    max_speed = run_df[\"speed\"].max()\n",
    "\n",
    "    dx = end_x_mirror_c - start_x_mirror_c\n",
    "    dy = end_y_mirror_c - start_y_mirror_c\n",
    "\n",
    "    run_angle_rad = np.arctan2(dy, dx)\n",
    "    run_angle_deg = np.degrees(run_angle_rad)\n",
    "\n",
    "    run_forward = dx > 0  # True if run is forward (right side of pitch)\n",
    "    \n",
    "    start_zone = get_zone(start_x_mirror_c, start_y_mirror_c, x_edges, y_edges)\n",
    "    end_zone = get_zone(end_x_mirror_c, end_y_mirror_c, x_edges, y_edges)\n",
    "\n",
    "    # Absolute pitch positions\n",
    "    start_x_abs = run_df[\"x\"].iloc[0]\n",
    "    start_y_abs = run_df[\"y\"].iloc[0]\n",
    "\n",
    "    end_x_abs = run_df[\"x\"].iloc[-1]\n",
    "    end_y_abs = run_df[\"y\"].iloc[-1]\n",
    "\n",
    "    start_zone_abs = get_zone(start_x_abs, start_y_abs, x_edges, y_edges)\n",
    "    end_zone_abs = get_zone(end_x_abs, end_y_abs, x_edges, y_edges)\n",
    "\n",
    "    phase_of_play = run_df[\"phase_of_play\"].iloc[0]\n",
    "    in_possession = run_df[\"in_possession\"].iloc[0]\n",
    "    team_role = run_df[\"team_role\"].iloc[0]\n",
    "    position = run_df[\"position\"].iloc[0]\n",
    "\n",
    "    # Extract ball carrier df\n",
    "    # ball_carrier_df = extract_ball_carrier_df_fast(\n",
    "    #     run_df, players_with_ball_df\n",
    "    # )\n",
    "    \n",
    "    # Check overlapping run\n",
    "    #overlapping = is_overlapping_run(run_df, ball_carrier_df)\n",
    "    match_id = run_df[\"match_id\"].iloc[0]\n",
    "    events_df = events_dfs_by_match.get(match_id)\n",
    "\n",
    "    # Ensure nested columns are unpacked\n",
    "    if events_df is not None:\n",
    "        if \"type.name\" not in events_df.columns:\n",
    "            events_df[\"type.name\"] = events_df[\"type\"].apply(\n",
    "                lambda d: d.get(\"name\") if isinstance(d, dict) else None\n",
    "            )\n",
    "        if \"team.name\" not in events_df.columns:\n",
    "            events_df[\"team.name\"] = events_df[\"team\"].apply(\n",
    "                lambda d: d.get(\"name\") if isinstance(d, dict) else None\n",
    "            )\n",
    "        if \"play_pattern.name\" not in events_df.columns:\n",
    "            events_df[\"play_pattern.name\"] = events_df[\"play_pattern\"].apply(\n",
    "                lambda d: d.get(\"name\") if isinstance(d, dict) else None\n",
    "            )\n",
    "        if \"pass.type.name\" not in events_df.columns:\n",
    "            events_df[\"pass.type.name\"] = events_df[\"pass\"].apply(\n",
    "                lambda d: d.get(\"type\", {}).get(\"name\") if isinstance(d, dict) else None\n",
    "            )\n",
    "        if \"player.id\" not in events_df.columns:\n",
    "            events_df[\"player.id\"] = events_df[\"player\"].apply(\n",
    "                lambda d: d.get(\"id\") if isinstance(d, dict) else None\n",
    "            )\n",
    "\n",
    "    overlapping = is_overlapping_run(run_df, events_df)\n",
    "    # underlapping = is_underlapping_run(run_df, ball_carrier_df)\n",
    "    # is_diag = is_diagonal_run(run_df)\n",
    "\n",
    "    zone_records.append({\n",
    "        \"run_id\": run_id,\n",
    "        \"start_zone\": start_zone,\n",
    "        \"end_zone\": end_zone,\n",
    "        \"start_zone_absolute\": start_zone_abs,\n",
    "        \"end_zone_absolute\": end_zone_abs,\n",
    "        \"phase_of_play\": phase_of_play,\n",
    "        \"in_possession\": in_possession,\n",
    "        \"team_role\": team_role,\n",
    "        \"position\": position,\n",
    "        \"run_length_m\": run_length,\n",
    "        \"mean_speed\": mean_speed,\n",
    "        \"max_speed\": max_speed,\n",
    "        \"run_angle_deg\": run_angle_deg,\n",
    "        \"run_forward\": run_forward,\n",
    "        \"tactical_overlap\": overlapping\n",
    "        # \"tactical_underlap\": underlapping,\n",
    "        # \"tactical_diagonal\": is_diag\n",
    "    })\n",
    "\n",
    "zones_df = pd.DataFrame(zone_records)\n",
    "print(zones_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = event_tracking_df[event_tracking_df[\"tracking_suffix\"].isna()]\n",
    "print(\"ðŸš« Events without tracking match:\")\n",
    "print(missing[[\"events_file\", \"team_name\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tracking_meta_df[[\"match_date\", \"home_team\", \"away_team\", \"home_team_full\", \"away_team_full\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_home = tracking_meta_df.loc[\n",
    "    tracking_meta_df[\"home_team\"].notna() &\n",
    "    tracking_meta_df[\"home_team_full\"].isna(),\n",
    "    \"home_team\"\n",
    "].unique()\n",
    "\n",
    "missing_away = tracking_meta_df.loc[\n",
    "    tracking_meta_df[\"away_team\"].notna() &\n",
    "    tracking_meta_df[\"away_team_full\"].isna(),\n",
    "    \"away_team\"\n",
    "].unique()\n",
    "\n",
    "print(\"Missing home team codes:\", missing_home)\n",
    "print(\"Missing away team codes:\", missing_away)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(event_meta_exploded[\"team_name\"].unique())\n",
    "print()\n",
    "print(print(tracking_meta_long[\"team_name\"].unique()))\n",
    "\n",
    "print(\"Number of unique teams in event metadata:\", len(event_meta_exploded[\"team_name\"].unique()))\n",
    "print(\"Number of unique teams in tracking metadata:\", len(tracking_meta_long[\"team_name\"].unique()))\n",
    "\n",
    "missing_in_tracking = set(event_meta_exploded[\"team_name\"].unique()) - set(tracking_meta_long[\"team_name\"].unique())\n",
    "print(\"Teams in events but missing in tracking:\", missing_in_tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
